[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Benjamin TOURNAN",
    "section": "",
    "text": "On this website, you can discover more about me, including my CV, explore various projects I have worked on and find numerous ways to contact me.\nI am a second-year master’s student pursuing a degree in anthropobiology in the vibrant city of Toulouse, France. I am passionate about employing bioinformatic tools to analyze and interpret biological data, contributing to a more comprehensive understanding of the intricate relationships within human evolution."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Benjamin TOURNAN",
    "section": "",
    "text": "I am a second-year master’s student pursuing a degree in anthropobiology in the vibrant city of Toulouse, France. My academic journey has provided me with a deep understanding of evolutionary processes, allowing me to explore the complexities of human evolution and adaptation. Throughout my studies, I have developed a keen interest in utilizing statistical and bioinformatics tools, particularly in the programming language R. I am passionate about employing these tools to analyze and interpret biological data, contributing to a more comprehensive understanding of the intricate relationships within human evolution."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Benjamin TOURNAN",
    "section": "Education",
    "text": "Education\n\nM.S. in Anthropobiology\nUniversity Paul Sabatier, Toulouse (France)  Sept 2022 - Present\nMy master’s degree in anthropobiology provided me with a comprehensive and interdisciplinary understanding of human evolution and biological diversity. The program covered a diverse array of fields, including paleoanthropology, which delves into the study of human ancestors and the evolutionary processes. Genomics and bioinformatics equipped me with cutting-edge tools to explore the genetic basis of human traits. Proficiency in statistical analysis and programming using R allowed me to critically analyze data and draw meaningful conclusions. Osteology, a fundamental component of the curriculum, provided a deep insight into skeletal anatomy and human remains. Additionally, the application of phylogenetic methods enabled me to trace evolutionary relationships across species. Finally, the incorporation of ecology and evolution broadened my perspective, connecting the intricate relationships between organisms and their environments. This diverse skill set and knowledge base have prepared me to engage in nuanced research and contribute to the ever-evolving field of anthropobiology.\n\n\nB.S. in Biology\nUniversity Paul Sabatier, Toulouse (France)  Sept 2019 - June 2022\nMy bachelor’s degree in biology, with a concentration on the biology of organisms, populations, and ecosystems, afforded me a comprehensive understanding of fundamental biological principles. The program extensively covered evolutionary processes, unraveling the intricacies of genetic variation, adaptation, and natural selection. General ecology studies provided insights into the dynamic interactions between organisms and their environments, encompassing both terrestrial and aquatic ecosystems. An introduction to statistical analysis using R equipped me with the skills to quantitatively assess biological phenomena, fostering a data-driven approach to research. Additionally, spatial analysis using QGIS allowed me to explore the spatial patterns and distributions of organisms and ecological processes. This well-rounded education has laid a solid foundation, enabling me to approach biological questions with a holistic perspective, integrating knowledge from molecular to ecological scales."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Benjamin TOURNAN",
    "section": "Experience",
    "text": "Experience\n\nCenter for Anthropobiology and Genomics of Toulouse (UMR 5288)\nMaster 2 internship | Feb 2024 - June 2024\nDuring this internship, my primary focus will revolve around conducting a 3D morphological analysis of dental and osteological remains from Australopithecus africanus.\n\n\nLaboratory of Evolution and Biological Diversity (UMR 5174)\nMaster 1 internship | April 2023 - June 2023\nDuring this internship, I delved into genetic diversity within Papua New Guinea by analyzing blood group alleles in a cohort of 150 individuals. The dissertation is entitled “Blood Group Systems Polymorphism in Papua New Guinea”. Employing statistical analyses with R and spatial visualization through QGIS, I unraveled compelling insights into the genetic landscape of Papua New Guinea. Furthermore, I will have the privilege to present my findings through a poster at the “Société d’Anthropologie de Paris” conference in January 2024.\n\n\nCenter for Anthropobiology and Genomics of Toulouse (UMR 5288)\nBachelor internship | June 2021\nThis first internship gave me an opportunity to discover the world of research and the basics of 3D imagery in Avizo.\n\n\nStudent job\nPublic swimming pool | Summers 2019 - 2023\nCash management, entry management, and training of new employees."
  },
  {
    "objectID": "index.html#title-1",
    "href": "index.html#title-1",
    "title": "Benjamin TOURNAN",
    "section": "",
    "text": "Lorem ipsum"
  },
  {
    "objectID": "index.html#title-2",
    "href": "index.html#title-2",
    "title": "Benjamin TOURNAN",
    "section": "Title 2",
    "text": "Title 2\nLorem ipsum"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Throughout my academic journey, I have undertaken and participated in numerous projects, some of which are showcased on this page. The first project entails a comprehensive document summarizing my proficiency in statistics, data visualization, and programming using R. The second project is a poster created to conclude my first-year master’s internship. The final project, a collaborative effort between my two colleagues and me during the second year of our master’s program, is a Shiny dashboard describing the genomic and historical aspects of the second Yersinia pestis pandemic in Europe.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials\n\n\n\n\n\n\n\n\n\n\n\n\n\nYersinia pestis dashboard\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoster: “Blood groups systems polymorphism in Papua New-Guinea”\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "R_mastery.html",
    "href": "R_mastery.html",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "",
    "text": "This comprehensive document contains all the information I have ever used in R, ranging from fundamental statistical analysis and basic graphical representations to more advanced visualizations using various packages and the essential principles of programming with R. Whether you are a beginner or an advanced user, this document has something for everyone.\nYou will find a wealth of information on statistical analysis, including descriptive statistics, hypothesis testing, regression analysis, and more. In addition to these basic concepts, I also cover more advanced topics such as data visualization with ggplot2, which will enable you to create complex and aesthetically pleasing graphs to better illustrate your data.\nMoreover, this document provides an overview of R programming essentials, including the data structures, functions, loops, and conditionals that are the building blocks of the language. By understanding these basic concepts, you can more easily manipulate and analyze data in R.\nTo ensure that this code can be reproduced if needed, I have included a chapter on “Reproducibility” at the end of this document. In this section, I provide a summary of every package I used, as well as their version, which can be useful if the code breaks. Overall, this document is a valuable resource for anyone who wants to improve their R programming skills and gain a deeper understanding of statistical analysis and data visualization."
  },
  {
    "objectID": "R_mastery.html#overview",
    "href": "R_mastery.html#overview",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "",
    "text": "This comprehensive document contains all the information I have ever used in R, ranging from fundamental statistical analysis and basic graphical representations to more advanced visualizations using various packages and the essential principles of programming with R. Whether you are a beginner or an advanced user, this document has something for everyone.\nYou will find a wealth of information on statistical analysis, including descriptive statistics, hypothesis testing, regression analysis, and more. In addition to these basic concepts, I also cover more advanced topics such as data visualization with ggplot2, which will enable you to create complex and aesthetically pleasing graphs to better illustrate your data.\nMoreover, this document provides an overview of R programming essentials, including the data structures, functions, loops, and conditionals that are the building blocks of the language. By understanding these basic concepts, you can more easily manipulate and analyze data in R.\nTo ensure that this code can be reproduced if needed, I have included a chapter on “Reproducibility” at the end of this document. In this section, I provide a summary of every package I used, as well as their version, which can be useful if the code breaks. Overall, this document is a valuable resource for anyone who wants to improve their R programming skills and gain a deeper understanding of statistical analysis and data visualization."
  },
  {
    "objectID": "R_mastery.html#data-manipulation",
    "href": "R_mastery.html#data-manipulation",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "Data manipulation",
    "text": "Data manipulation\nTo begin with R it is primordial to know your way around a data frame, in this section I will go over the functions needed to shape your date like you need it.\nBase R\nThe most basic manipulation and overview functions are directly in R without any package necessary.\nImport your data\nThe first step is to import your data. To import data into R, there are several functions you can use, including read.table(), read.csv(), read.csv2(), read.delim(), read.delim2(), and more. Here’s a brief explanation of each function:\n\n\nread.table(): This function is used to read data from a text file. It is a very versatile function that can handle many different file formats, but it requires you to specify some arguments such as the file path, the delimiter used in the file, and if the file contains a header or not.\n\nread.csv(): This function is similar to read.table(), but it is specifically designed to read comma-separated value (CSV) files. It assumes that the file has a header row, and it automatically sets the delimiter to a comma.\n\nread.csv2(): This function is similar to read.csv(), but it is specifically designed to read CSV files where the decimal separator is a semicolon (;) and the field separator is a comma (,).\n\nread.delim(): This function is similar to read.csv(), but it is specifically designed to read tab-delimited files.\n\nread.delim2(): This function is similar to read.delim(), but it is specifically designed to read tab-delimited files where the decimal separator is a semicolon (;) and the field separator is a tab (\\t). For example, to read a CSV file named “data.csv” in your current working directory, you can use the read.csv function as follows:\n\n\ndata &lt;- read.csv(\"data.csv\")\n\nThis assumes that the file has a header row, uses a comma as the delimiter, and that the file is located in your current working directory.\nYou can adjust the parameters of these functions based on the specifics of your data and the file format. For more information, you can use the R help system by typing ?read.csv, ?read.table, etc.\nWhat does your data look like?\nIn this section I will give you some of the most commonly used functions in R for exploring and summarizing data.\nThe function head() is used to display the first few rows of a data frame. By default, it displays the first six rows, but you can specify the number of rows you want to display.\n\ndata(iris) # load the iris dataset\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nThe function summary() is used to generate a summary of a data frame, including measures of central tendency and variability for each variable. It is particularly useful for identifying missing values, outliers, and the distribution of the data.\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width          Species  \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100   setosa    :50  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300   versicolor:50  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300   virginica :50  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199                  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800                  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500                  \n\n\nThe str() function is used to display the structure of a data frame, including the number of observations and variables, their names, data types, and a preview of the data. It is particularly useful for identifying the format of the data and for checking if variables are of the right data type.\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nThe colSums(is.na(data)) function is used to check for missing values in a data frame. It returns the total number of missing values per column. It is particularly useful for identifying if any variables have missing data and for imputing missing values.\n\ncolSums(is.na(iris))\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n           0            0            0            0            0 \n\n\nBasic functions\nTo effectively modify and customize your data frames, you’ll need to utilize a variety of functions available in R. Fortunately, there are numerous options available to help you achieve your desired results. Below are some examples of functions you may find helpful in your data frame manipulations.\nOne of the most useful function is subset(). This function allows you to subset a data frame based on one or more conditions. Here’s an example of using subset() to select only the rows of the iris dataframe where the value in the Sepal.Length column is greater than 5:\n\niris_subset &lt;- subset(iris, Sepal.Length &gt; 5)\nhead(iris_subset)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n11          5.4         3.7          1.5         0.2  setosa\n15          5.8         4.0          1.2         0.2  setosa\n16          5.7         4.4          1.5         0.4  setosa\n17          5.4         3.9          1.3         0.4  setosa\n\n\nThe function merge() allows you to merge two or more data frames based on a common variable. Here’s an example of using merge() to combine two data frames based on the common ID column:\n\n# create two sample data frames\ndf1 &lt;- data.frame(id = c(1, 2, 3), x = c(4, 5, 6))\ndf2 &lt;- data.frame(id = c(2, 3, 4), y = c(7, 8, 9))\n\n# use merge to combine the data frames based on the \"id\" column\nmerged_df &lt;- merge(df1, df2, by = \"id\")\nmerged_df\n\n  id x y\n1  2 5 7\n2  3 6 8\n\n\nThe function apply() can be used to apply a function to rows or columns of a data frame. In this example, the code applies the sum function to each row of the data frame, if you wanted to sum each column you can change (1) to (2).\n\n# create a sample data frame\ndf &lt;- data.frame(x = c(1, 2, 3), \n                 y = c(4, 5, 6), \n                 z = c(7, 8, 9))\n\n# use apply to calculate the row sums\nrow_sums &lt;- apply(df, 1, sum)\nrow_sums\n\n[1] 12 15 18\n\n\nThe tapply() function can be used to apply a function to subsets of a data frame defined by one or more variables. Here’s an example of using tapply() to calculate the mean of a variable for each level of a grouping variable.\n\n# create a sample data frame\ndf &lt;- data.frame(x = c(1, 2, 3, 4), \n                 y = c(5, 6, 7, 8), \n                 group = c(\"A\", \"A\", \"B\", \"B\"))\n\n# use tapply to calculate the mean of y for each group\nmean_by_group &lt;- tapply(df$y, df$group, mean)\nmean_by_group\n\n  A   B \n5.5 7.5 \n\n\nAnother function from the apply environment is lapply(). This function can be used to apply a function to each column of a data frame and return a list of the results. Here’s an example of using lapply() to calculate the range of values for each column of a data frame:\n\n# create a sample data frame\ndf &lt;- data.frame(x = c(1, 2, 3), \n                 y = c(4, 5, 6), \n                 z = c(7, 8, 9))\n\n# use lapply to calculate the range of values for each column\nrange_by_col &lt;- lapply(df, range)\nrange_by_col\n\n$x\n[1] 1 3\n\n$y\n[1] 4 6\n\n$z\n[1] 7 9\n\n\nIf you want to simply the output to a vector (or matrix if possible) you can use the sapply() function. In this example we calculate the mean of each column of a data frame:\n\n# create a sample data frame\ndf &lt;- data.frame(x = c(1, 2, 3), \n                 y = c(4, 5, 6), \n                 z = c(7, 8, 9))\n\n# use sapply to calculate the mean of each column\nmean_by_col &lt;- sapply(df, mean)\nmean_by_col\n\nx y z \n2 5 8 \n\n\nIf you want to change the column names, you can use the colnames() function like in this example:\n\n# Create a sample data frame\ndf &lt;- data.frame(x = c(1, 2, 3), \n                 y = c(4, 5, 6))\n\n# Set column names to be \"Var1\" and \"Var2\"\ncolnames(df) &lt;- c(\"Var1\", \"Var2\")\n\n# Print the data frame with column names changed\ndf\n\n  Var1 Var2\n1    1    4\n2    2    5\n3    3    6\n\n\nYou can also change the row names with the rownames() function as displayed here:\n\n# Create a sample data frame\ndf &lt;- data.frame(x = c(1, 2, 3), \n                 y = c(4, 5, 6))\n\n# Set row names to be \"A\", \"B\", and \"C\"\nrow.names(df) &lt;- c(\"A\", \"B\", \"C\")\n\n# Print the data frame with row names changed\ndf\n\n  x y\nA 1 4\nB 2 5\nC 3 6\n\n\nMoving to the tidyverse\nThe tidyverse is a collection of R packages that are designed to work together to make data analysis more efficient and intuitive. The packages in the tidyverse include:\n\n\nggplot2 for creating visualizations\n\ndplyr for data manipulation\n\ntidyr for reshaping data\n\nreadr for importing data\n\npurrr for functional programming\n\ntibble for creating data frames\n\n\nlibrary(tidyverse)\n\nIn the tidyverse something very useful is the pipe operator: %&gt;%, it allows you to chain together multiple functions in a way that makes the code easier to read and understand. It works by taking the output of one function and passing it as the first argument to the next function in the chain. This eliminates the need to create intermediate variables to store the output of each function, and makes it easier to write code that is both concise and expressive.\nThe readr package\nThis package provides a fast and friendly way to read in flat files, including CSV, TSV, and fixed-width files. It is designed to be faster and more memory-efficient than the base R functions for reading in data.\nThe package provides several functions for reading in data, including:\n\n\nread_csv(): for reading in CSV files\n\nread_tsv(): for reading in TSV files\n\nread_delim(): for reading in files with a delimiter of your choice\n\nread_fwf(): for reading in fixed-width files\n\nThese functions are designed to be fast and memory-efficient, and they also automatically detect and handle a wide range of data formats and encodings.\nOne of the key features of readr is its ability to automatically detect the types of columns in your data, such as numeric, character, or date/time columns. However, if you have specific requirements for the types of columns in your data, you can also specify the column types manually using the col_types argument.\nFor example, if you want to specify that the second column in your data is a character column and the fourth column is a numeric column, you can do:\n\nmy_data &lt;- read_csv(\"my_data.csv\", col_types = cols(\n  col2 = col_character(),\n  col4 = col_double()\n))\n\nreadr provides several options for handling missing values in your data. By default, readr will treat any value that is empty or matches the string “NA” as a missing value.\nYou can also specify additional strings to be treated as missing values using the na argument. For example, if you want to treat the string “MISSING” as a missing value, you can do:\n\nmy_data &lt;- read_csv(\"my_data.csv\", na = c(\"NA\", \"MISSING\"))\n\nThe dplyr package\ndplyr is a powerful and widely-used package in R for data manipulation. It provides a concise and consistent syntax for transforming data sets, which allows you to easily filter, arrange, summarize, and join data.\nHere’s a brief overview of the main functions in dplyr:\n\n\nselect(): Allows you to select specific columns from a data set, using a simple syntax that includes the column names separated by commas.\n\nfilter(): Allows you to select specific rows from a data set based on a set of conditions, using a simple syntax that includes logical operators such as ==, &gt;, &lt;, etc.\n\narrange(): Allows you to sort a data set based on one or more columns, using a simple syntax that includes the column names separated by commas, with optional modifiers like desc() for descending order.\n\nmutate(): Allows you to create new columns in a data set based on calculations or transformations of existing columns, using a simple syntax that includes the name of the new column followed by an equals sign (=) and the calculation or transformation expression.\n\nsummarize(): Allows you to aggregate data by computing summary statistics such as mean, median, count, etc., using a simple syntax that includes the summary function name followed by the name of the column to be summarized.\n\ngroup_by(): Allows you to group a data set by one or more columns, which is often used in conjunction with summarize() to compute summary statistics for each group.\n\njoin(): Allows you to combine two or more data sets based on common columns, using a simple syntax that includes the names of the data sets and the columns to be joined on.\n\nHere’s an example of how to use dplyr to perform some common data manipulation tasks:\n\nlibrary(dplyr)\n\n# load a sample data set\nmy_data &lt;- iris\n\n# select specific columns\nmy_data &lt;- select(my_data, Sepal.Length, Sepal.Width, Species)\n\n# filter rows based on a condition\nmy_data &lt;- filter(my_data, Sepal.Length &gt; 5)\n\n# arrange data by a column\nmy_data &lt;- arrange(my_data, Sepal.Length)\n\n# create a new column based on an existing one\nmy_data &lt;- mutate(my_data, Sepal.Area = Sepal.Length * Sepal.Width)\n\n# summarize data by a column\nmy_summary &lt;- summarise(my_data, Mean.Sepal.Length = mean(Sepal.Length))\n\n# group data by a column and summarize\nmy_summary2 &lt;- my_data %&gt;%\n  group_by(Species) %&gt;%\n  summarise(Mean.Sepal.Length = mean(Sepal.Length))\n\n# join two data sets\nmy_data2 &lt;- data.frame(Species = c(\"setosa\", \"versicolor\", \"virginica\"),\n                       Petal.Length = c(1.4, 4.2, 6.0))\nmy_data &lt;- left_join(my_data, my_data2, by = \"Species\")\n\nIn this example, we load the iris data set, which contains information about flowers, and use various dplyr functions to select specific columns, filter rows based on a condition, arrange the data by a column, create a new column based on an existing one, summarize the data by a column, group the data by a column and summarize, and join two data sets based on a common column.\nOverall, dplyr provides a powerful and flexible set of tools for data manipulation in R, and is a key package to have in your data science toolkit.\nThe tidyr package\nThe tidyr package is a data manipulation package in R that provides functions for reshaping and tidying data sets. It is a complementary package to dplyr, which focuses on manipulating data sets by filtering, summarizing, and transforming variables.\nThe gather() function is used to reshape a data frame from a wide format to a long format. It does this by taking multiple columns and “gathering” them into a key-value pair format. For example, if you have a data frame with columns for year, quarter, and sales for each quarter, you can use gather() to reshape it into a format where each row represents a year-quarter combination and the sales value is in a single column.\n\n# create example data frame\ndf &lt;- data.frame(year = c(2018, 2019),\n                 q1_sales = c(100, 200),\n                 q2_sales = c(150, 250),\n                 q3_sales = c(175, 300),\n                 q4_sales = c(200, 350))\n\n# reshape data frame\ndf2 &lt;- gather(df, key = \"quarter\", value = \"sales\", -year)\n\nThe function spread() is the opposite of gather(). It is used to reshape a data frame from a long format to a wide format. It does this by taking a key-value pair and “spreading” the values across multiple columns. For example, if you have a data frame with columns for year, quarter, and sales, where each row represents a year-quarter combination, you can use spread() to reshape it into a format where each row represents a year and there is a column for each quarter’s sales value.\n\n# create example data frame\ndf &lt;- data.frame(month = c(\"Jan\", \"Feb\", \"Mar\", \"Jan\", \"Feb\", \"Mar\"),\n                 year = c(2018, 2018, 2018, 2019, 2019, 2019),\n                 rainfall = c(50, 75, 100, 60, 80, 110))\n\n# reshape data frame\ndf2 &lt;- spread(df, key = \"month\", value = \"rainfall\")\n\nThe separate() function is used to split a single column into multiple columns based on a delimiter or a fixed number of characters. For example, if you have a data frame with a column for name that contains both first and last names, you can use separate() to split the column into two columns.\n\n# create example data frame\ndf &lt;- data.frame(name = c(\"John Smith\", \"Jane Doe\", \"Bob Johnson\"))\n\n# separate name column into first and last name columns\ndf2 &lt;- separate(df, col = name, into = c(\"first_name\", \"last_name\"), sep = \" \")\n\nThe function unite() is the opposite of separate(). It is used to combine multiple columns into a single column by concatenating them with a delimiter. For example, if you have a data frame with separate columns for first and last names, you can use unite() to combine them into a single column.\n\n# create example data frame\ndf &lt;- data.frame(first_name = c(\"John\", \"Jane\", \"Bob\"),\n                 last_name = c(\"Smith\", \"Doe\", \"Johnson\"))\n\n# combine first and last name columns into a single name column\ndf2 &lt;- unite(df, col = \"name\", first_name, last_name, sep = \" \")\n\nThe fill() function is used to fill in missing values in a data frame. It does this by filling in missing values with the most recent non-missing value in the same column. For example, if you have a data frame with a column for price that has missing values, you can use fill() to fill in those missing values with the most recent non-missing price value.\n\n# create example data frame\ndf &lt;- data.frame(date = c(\"2022-01-01\", \"2022-01-02\", \"2022-01-03\", \"2022-01-04\"),\n                 price = c(100, NA, 150, NA))\n\n# fill in missing values with most recent non-missing value\ndf2 &lt;- fill(df, price)\ndf2\n\n        date price\n1 2022-01-01   100\n2 2022-01-02   100\n3 2022-01-03   150\n4 2022-01-04   150\n\n\ndrop_na() is a useful function when we have a data frame with missing values, and we want to remove any rows that contain missing values.\n\n# create example data frame\ndf &lt;- data.frame(id = 1:5,\n                 name = c(\"John\", NA, \"Bob\", \"Jane\", \"Bill\"),\n                 age = c(30, 25, NA, 40, 35))\n\n# drop any rows that contain missing values\ndf2 &lt;- drop_na(df)\n\nOverall, tidyr is a powerful package that provides a range of functions for tidying and reshaping data frames. It is a key component in the “tidyverse” of packages in R that are designed to work together to provide a consistent and efficient data analysis workflow.\nThe tibble package\nThe tibble package is an alternative to R’s built-in data.frame object. It is designed to be more user-friendly and to make working with data frames easier. Some of the key features of tibble include:\n\nPrinting: tibble objects print nicely to the console, making them easier to read.\nError messages: tibble objects have improved error messages that provide more context than the default data.frame error messages.\nSubsetting: tibble objects behave more consistently than data.frame objects when subsetting.\nColumn types: tibble objects preserve column types better than data.frame objects.\n\nHere is an example of how to create and work with a tibble object:\n\n# create a tibble with three columns\nmy_data &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  has_pet = c(TRUE, FALSE, TRUE)\n)\n\n# print the tibble to the console\nmy_data\n\n# A tibble: 3 × 3\n  name      age has_pet\n  &lt;chr&gt;   &lt;dbl&gt; &lt;lgl&gt;  \n1 Alice      25 TRUE   \n2 Bob        30 FALSE  \n3 Charlie    35 TRUE   \n\n# filter the tibble to only include people with pets\nmy_data_with_pets &lt;- filter(my_data, has_pet == TRUE)\n\n# add a new column to the tibble\nmy_data_with_pets &lt;- mutate(my_data_with_pets, pet_type = c(\"dog\", \"cat\"))\n\n# print the modified tibble to the console\nmy_data_with_pets\n\n# A tibble: 2 × 4\n  name      age has_pet pet_type\n  &lt;chr&gt;   &lt;dbl&gt; &lt;lgl&gt;   &lt;chr&gt;   \n1 Alice      25 TRUE    dog     \n2 Charlie    35 TRUE    cat     \n\n\nIn this example, we create a tibble object called my_data with three columns: name, age, and has_pet. We then print the tibble to the console, which shows the nicely formatted output. We use filter() to create a new tibble called my_data_with_pets that only includes rows where has_pet is TRUE. We then use mutate() to add a new column called pet_type to my_data_with_pets. Finally, we print the modified tibble to the console, which shows the added pet_type column.\nOther packages\nggplot2 is an R package for data visualization that is based on the grammar of graphics, which provides a consistent framework for building graphics by breaking them down into components. The package allows for a high degree of flexibility and customization, allowing users to create a wide range of visualizations, from simple scatterplots to complex multi-layered graphics. ggplot2 works by mapping data to aesthetic properties of the plot, such as color, shape, and size, and then layering geometric objects, such as points, lines, and bars, on top of this mapping. This allows for easy manipulation and comparison of different aspects of the data. The package also provides many tools for modifying and fine-tuning the appearance of the plot, such as changing axis labels and limits, adjusting colors and themes, and adding annotations. Overall, ggplot2 is a powerful and flexible tool for creating high-quality visualizations in R. We will dive deeper in ggplot2 in a future chapter.\nThe other tydiverse package not aforementioned is the purrr package. It is a powerful and flexible package for working with functions and vectors in R. It provides a consistent set of tools for manipulating and transforming data in a functional programming style. Some of the key features of purrr include the ability to work with functions in a more flexible way, including functions with multiple arguments and complex inputs, as well as the ability to apply functions to data in a variety of ways, such as mapping over lists and data frames. purrr also provides many functions for working with vectors, including tools for filtering, grouping, and reshaping data, as well as for handling missing values and dealing with errors. Overall, purrr is a powerful and flexible package that provides a consistent and functional approach to working with data in R."
  },
  {
    "objectID": "R_mastery.html#statistical-analysis",
    "href": "R_mastery.html#statistical-analysis",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "Statistical analysis",
    "text": "Statistical analysis\nIn this chapter we will focus on statistical tests, how to use them but also interpret them.\nTest for normality and transformations\nIn statistical analysis, it is often essential to check whether a dataset follows a normal distribution before applying certain parametric tests. R provides various tools to assess the normality of data and apply transformations if necessary. In this section, we will explore some common methods to test for normality and demonstrate how to perform data transformations.\nShapiro-Wilk Test\nThe Shapiro-Wilk test is a popular method to assess normality. It tests the null hypothesis that a sample is drawn from a normally distributed population. We can use the shapiro.test() function in R to perform this test. Let’s see an example:\n\n# Generate a random sample from a normal distribution\nset.seed(123)\nnormal_sample &lt;- rnorm(100)\n\n# Perform Shapiro-Wilk test\nshapiro.test(normal_sample)\n\n\n    Shapiro-Wilk normality test\n\ndata:  normal_sample\nW = 0.99388, p-value = 0.9349\n\n\nHistogram and Q-Q Plot\nAnother visual approach to check for normality is by creating a histogram and a quantile-quantile (Q-Q) plot of the data. The histogram provides a rough representation of the data’s distribution, while the Q-Q plot compares the quantiles of the data to the quantiles of a theoretical normal distribution. If the points on the Q-Q plot fall approximately along a straight line, it suggests the data is normally distributed. Here’s an example:\n\n# Generate a random sample from a non-normal distribution\nset.seed(456)\nnon_normal_sample &lt;- rgamma(100, shape = 2, rate = 1)\n\n# Create a histogram\nhist(non_normal_sample, main = \"Histogram of Non-Normal Data\")\n\n\n\n# Create a Q-Q plot\nqqnorm(non_normal_sample)\nqqline(non_normal_sample, col = \"red\")\n\n\n\n\nData Transformation\nIf the data significantly deviates from normality, we can try applying transformations to make it approximately normal. Common transformations include logarithmic, square root, and reciprocal transformations. Let’s apply a logarithmic transformation to our non-normal data and re-check for normality:\n\n# Apply logarithmic transformation\nlog_transformed_data &lt;- log(non_normal_sample)\n\n# Create a histogram of transformed data\nhist(log_transformed_data, main = \"Histogram of Log-Transformed Data\")\n\n\n\n# Create a Q-Q plot of transformed data\nqqnorm(log_transformed_data)\nqqline(log_transformed_data, col = \"red\")\n\n\n\n# Perform Shapiro-Wilk test on transformed data\nshapiro.test(log_transformed_data)\n\n\n    Shapiro-Wilk normality test\n\ndata:  log_transformed_data\nW = 0.95742, p-value = 0.002644\n\n\nRemember that data transformations should be applied judiciously and be interpreted cautiously in the context of your specific analysis.\nIn conclusion, R offers powerful tools to assess the normality of data and apply appropriate transformations to meet the assumptions of parametric tests. Always validate the results and consider the specific requirements of your analysis before making any conclusions based on the normality tests and transformations.\nThe Chi-square test of independence\nThe Chi-square test of independence is a fundamental statistical method used to determine whether there is a significant association between two categorical variables. It is applicable when we have two or more categorical variables, and we want to investigate if they are dependent or independent of each other. The test assesses whether the observed frequencies in a contingency table differ significantly from the frequencies we would expect under the assumption of independence between the variables.\nLet’s go through an example using R to illustrate the Chi-square test of independence.\nSuppose we have survey data on the preference for ice cream flavors among two groups of people: Group A and Group B. The data is organized in a contingency table:\n\n\n\nVanilla\nChocolate\nStrawberry\n\n\n\nGroup A\n25\n15\n10\n\n\nGroup B\n30\n20\n25\n\n\n\nWe can use the chisq.test() function in R to perform the Chi-square test:\n\n# Create the contingency table\nice_cream_table &lt;- matrix(c(25, 15, 10, 30, 20, 25), nrow = 2, byrow = TRUE,\n                          dimnames = list(c(\"Group A\", \"Group B\"),\n                                          c(\"Vanilla\", \"Chocolate\", \"Strawberry\")))\n\n# Perform the Chi-square test\nchi_sq_result &lt;- chisq.test(ice_cream_table)\nprint(chi_sq_result)\n\n\n    Pearson's Chi-squared test\n\ndata:  ice_cream_table\nX-squared = 2.7056, df = 2, p-value = 0.2585\n\n\nThe test will provide us with a chi-square statistic and a p-value. If the p-value is below a predetermined significance level (often set at 0.05), we reject the null hypothesis of independence and conclude that there is a significant association between the two categorical variables. Conversely, if the p-value is greater than 0.05, we fail to reject the null hypothesis, suggesting that there is no significant association between the variables.\nIn summary, the Chi-square test of independence is a valuable tool to assess the relationship between categorical variables. It is commonly used in various fields, including social sciences, medicine, marketing, and more, to gain insights into the associations between different groups and make informed decisions based on the observed data.\nCorrelation tests\nCorrelation tests are essential statistical techniques used to assess the strength and direction of the relationship between two continuous variables. In R, we can employ various correlation tests to determine if there is a significant linear association between the variables. Two commonly used correlation tests are the Pearson correlation coefficient and the Spearman rank correlation coefficient.\nPearson Correlation Coefficient\nThe Pearson correlation coefficient, also known as Pearson’s r, measures the linear relationship between two continuous variables. It ranges from -1 to 1, where -1 indicates a perfect negative linear relationship, 1 indicates a perfect positive linear relationship, and 0 suggests no linear relationship. The cor() function in R can be used to compute Pearson’s correlation coefficient. Here’s an example:\n\n# Generate sample data\nset.seed(789)\nvariable1 &lt;- rnorm(100)\nvariable2 &lt;- rnorm(100)\n\n# Calculate Pearson correlation\ncor(variable1, variable2, method = \"pearson\")\n\n[1] -0.05680459\n\n\nSpearman Rank Correlation Coefficient\nThe Spearman rank correlation coefficient assesses the strength and direction of a monotonic relationship between two variables. It is suitable for variables that may not have a linear relationship but exhibit a consistent monotonic trend. The cor() function with method = \"spearman\" can be used to compute Spearman’s correlation coefficient. Here’s an example:\n\n# Generate sample data\nset.seed(987)\nvariable3 &lt;- runif(100)\nvariable4 &lt;- runif(100)\n\n# Calculate Spearman correlation\ncor(variable3, variable4, method = \"spearman\")\n\n[1] -0.1977798\n\n\nIn both cases, the correlation tests will produce a correlation coefficient and a p-value. The p-value indicates the statistical significance of the correlation. A p-value less than the chosen significance level (often 0.05) suggests that the correlation is statistically significant.\nUnderstanding the relationship between variables is crucial in data analysis and can provide valuable insights for decision-making and model building. Correlation tests help us quantify the association between variables, enabling us to make informed conclusions about their interdependence in the dataset.\nLinear regression\nLinear regression is a powerful statistical technique used to model the relationship between a dependent variable and one or more independent variables. It is widely employed in various fields, including economics, social sciences, and machine learning. In R, we can use the lm() function to perform linear regression and estimate the coefficients of the regression equation. Let’s explore three aspects of linear regression: simple linear regression, multiple linear regression, and variable selection with model comparison.\nSimple linear regression\nSimple linear regression involves modeling the relationship between two continuous variables: a dependent variable and a single independent variable. It assumes that the relationship can be approximated by a straight line equation (y = mx + b), where y represents the dependent variable, x is the independent variable, m is the slope, and b is the intercept. We can use the lm() function to perform simple linear regression in R. Here’s an example:\n\n# Generate sample data\nset.seed(456)\nindependent_var &lt;- rnorm(100)\ndependent_var &lt;- 2 * independent_var + rnorm(100)\n\n# Perform simple linear regression\nsimple_lm_result &lt;- lm(dependent_var ~ independent_var)\nsummary(simple_lm_result)\n\n\nCall:\nlm(formula = dependent_var ~ independent_var)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.21548 -0.81266 -0.04914  0.91531  1.91311 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -0.11472    0.09834  -1.166    0.246    \nindependent_var  1.98347    0.09797  20.246   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9763 on 98 degrees of freedom\nMultiple R-squared:  0.807, Adjusted R-squared:  0.8051 \nF-statistic: 409.9 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nThe output of the summary() function will provide information about the following key components:\n\nCoefficients Table:\nThe coefficients table shows the estimated regression coefficients for the intercept ((Intercept)) and the independent variable (independent_var). The coefficient estimates represent the change in the dependent variable (dependent_var) for a one-unit increase in the independent variable while holding other variables constant.\nStandard Error:\nThe standard error is an estimate of the variability or uncertainty associated with the coefficient estimates. Smaller standard errors indicate more precise estimates.\nt-value:\nThe t-value measures how many standard errors the coefficient estimate is away from zero. It is used to test the null hypothesis that the true coefficient is zero (i.e., no relationship between the independent and dependent variables). A high t-value indicates a significant relationship between the variables.\nPr(&gt;|t|):\nThe p-value associated with the t-value. It indicates the probability of observing the t-value or more extreme values under the null hypothesis. A p-value below the chosen significance level (often 0.05) suggests a statistically significant relationship.\nResidual Standard Error (RSE):\nThe residual standard error is an estimate of the standard deviation of the residuals (the differences between the observed and predicted values). It measures the goodness of fit of the model - smaller RSE indicates a better fit.\nR-squared (R2):\nThe R-squared value represents the proportion of variance in the dependent variable explained by the independent variable(s). It ranges from 0 to 1, with higher values indicating a better fit. For simple linear regression, R-squared is equal to the square of the correlation coefficient between the dependent and independent variables.\nMultiple linear regression\nMultiple linear regression extends the concept of simple linear regression to include multiple independent variables. It models the relationship between a dependent variable and two or more independent variables, assuming a linear combination of these variables to predict the dependent variable. The lm() function can also be used for multiple linear regression. Let’s see an example:\n\n# Generate sample data\nset.seed(789)\nindependent_var1 &lt;- rnorm(100)\nindependent_var2 &lt;- rnorm(100)\ndependent_var_multiple &lt;- 2 * independent_var1 + 3 * independent_var2 + rnorm(100)\n\n# Perform multiple linear regression\nmultiple_lm_result &lt;- lm(dependent_var_multiple ~ independent_var1 + independent_var2)\nsummary(multiple_lm_result)\n\n\nCall:\nlm(formula = dependent_var_multiple ~ independent_var1 + independent_var2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1398 -0.6257  0.1290  0.6580  2.2478 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.09354    0.09893  -0.946    0.347    \nindependent_var1  2.08573    0.10162  20.524   &lt;2e-16 ***\nindependent_var2  2.95072    0.09538  30.937   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.986 on 97 degrees of freedom\nMultiple R-squared:  0.9311,    Adjusted R-squared:  0.9297 \nF-statistic: 655.2 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nThe summary() output provides coefficient estimates for each independent variable, along with their standard errors, t-values, and p-values. Additionally, it gives an overall assessment of the model’s fit.\nThe summary table for multiple linear regression is similar to the one for simple linear regression. However, in multiple linear regression, the coefficients table will contain estimates for each independent variable, along with the intercept.\nAdditionally, you will find the following:\n\nMultiple R-squared:\nThe multiple R-squared (R2) indicates the proportion of variance in the dependent variable explained by all the independent variables in the model.\nAdjusted R-squared:\nThe adjusted R-squared adjusts the multiple R-squared value based on the number of independent variables and the sample size. It penalizes the addition of unnecessary variables to prevent overfitting. A higher adjusted R-squared suggests a better-fitting model.\n\nThe summary() output also includes information on residuals and various diagnostics that can be useful for evaluating the model’s assumptions and checking for potential issues like heteroscedasticity or influential outliers.\nRemember that while the summary table provides useful information about the model, it is essential to consider other diagnostic plots and statistical tests to ensure the model’s validity and interpret the results properly.\nVariable selection and model comparison\nIn linear regression, selecting the most relevant independent variables is essential to build a robust and interpretable model. There are various approaches to perform variable selection, such as forward selection, backward elimination, and stepwise regression. Additionally, model comparison helps us evaluate different models and choose the best-fit model based on several metrics, including R-squared, adjusted R-squared, AIC (Akaike Information Criterion), and BIC (Bayesian Information Criterion).\nLet’s consider a dataset with multiple potential independent variables. We can use different variable selection techniques to identify the subset of variables that contribute most significantly to the model. In this example, we’ll use the step() function in R to perform a stepwise regression for variable selection:\n\n# Load necessary library for stepwise regression\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n# Generate sample data\nset.seed(123)\nindependent_var1 &lt;- rnorm(100)\nindependent_var2 &lt;- rnorm(100)\ndependent_var &lt;- 3 * independent_var1 + 2 * independent_var2 + rnorm(100)\n\n# Perform stepwise regression for variable selection\nmodel &lt;- lm(dependent_var ~ independent_var1 + independent_var2)\nselected_model &lt;- step(model)\n\nStart:  AIC=-7.03\ndependent_var ~ independent_var1 + independent_var2\n\n                   Df Sum of Sq    RSS     AIC\n&lt;none&gt;                           87.78  -7.032\n- independent_var2  1    378.22 466.01 157.903\n- independent_var1  1    676.30 764.08 207.350\n\nsummary(selected_model)\n\n\nCall:\nlm(formula = dependent_var ~ independent_var1 + independent_var2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8730 -0.6607 -0.1245  0.6214  2.0798 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.13507    0.09614   1.405    0.163    \nindependent_var1  2.86683    0.10487  27.337   &lt;2e-16 ***\nindependent_var2  2.02381    0.09899  20.444   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9513 on 97 degrees of freedom\nMultiple R-squared:  0.9198,    Adjusted R-squared:  0.9182 \nF-statistic: 556.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nThe step() function will perform a stepwise selection and provide the final selected model with the most significant variables.\nOnce we have multiple candidate models, we need to evaluate their performance to choose the best one. Several metrics can be used for model comparison:\nR-squared (R^2)\nR-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model. A higher R-squared value indicates a better fit of the model to the data.\n\n# Calculate R-squared for the selected model\nrsquared &lt;- summary(selected_model)$r.squared\nprint(paste(\"R-squared:\", round(rsquared, 3)))\n\n[1] \"R-squared: 0.92\"\n\n\nAdjusted R-squared (R^2_adj)\nAdjusted R-squared is a modified version of R-squared that considers the number of independent variables in the model. It penalizes the inclusion of irrelevant variables, providing a more realistic assessment of model fit.\n\n# Calculate Adjusted R-squared for the selected model\nrsquared_adj &lt;- summary(selected_model)$adj.r.squared\nprint(paste(\"Adjusted R-squared:\", round(rsquared_adj, 3)))\n\n[1] \"Adjusted R-squared: 0.918\"\n\n\nAkaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)\nAIC and BIC are information criteria that balance the goodness of fit with model complexity. Lower AIC and BIC values indicate a better model fit with fewer variables, making them useful for model comparison.\n\naic &lt;- AIC(selected_model)\nprint(paste(\"AIC:\", round(aic, 3)))\n\n[1] \"AIC: 278.756\"\n\nbic &lt;- BIC(selected_model)\nprint(paste(\"BIC:\", round(bic, 3)))\n\n[1] \"BIC: 289.177\"\n\n\nBy comparing these metrics across different candidate models, we can select the model that provides the best trade-off between goodness of fit and simplicity, ensuring a reliable representation of the relationship between the variables.\nIn conclusion, variable selection and model comparison are critical steps in building an effective linear regression model. Properly selected models, based on meaningful metrics such as R-squared, adjusted R-squared, AIC, and BIC, can lead to better insights and more accurate predictions. Always interpret the results with caution and consider the context and assumptions of the analysis.\nValidity of the model\nLinear regression makes several assumptions about the data, such as :\n\nLinearity of the data. The relationship between the predictor (x) and the outcome (y) is assumed to be linear.\nNormality of residuals. The residual errors are assumed to be normally distributed.\nHomogeneity of residuals variance. The residuals are assumed to have a constant variance (homoscedasticity)\nIndependence of residuals error terms.\n\nDiagnostic Plot: Q-Q Plot\nThe Q-Q (quantile-quantile) plot helps us assess the assumption of normality in the residuals. If the residuals follow a normal distribution, the points on the Q-Q plot should approximately fall along a straight line.\n\n# Create a Q-Q plot of residuals\nqqnorm(simple_lm_result$residuals, main = \"Q-Q Plot of Residuals\")\nqqline(simple_lm_result$residuals, col = \"red\")\n\n\n\n\nStatistical Test: Shapiro-Wilk Test\nTo complement the Q-Q plot, we can perform the Shapiro-Wilk test to formally test for the normality of the residuals. The null hypothesis is that the residuals are normally distributed. A significant p-value (p &lt; 0.05) indicates a departure from normality.\n\n# Shapiro-Wilk test for normality of residuals\nshapiro_test &lt;- shapiro.test(simple_lm_result$residuals)\nprint(shapiro_test)\n\n\n    Shapiro-Wilk normality test\n\ndata:  simple_lm_result$residuals\nW = 0.96982, p-value = 0.0214\n\n\nStatistical Test: Durbin-Watson Test\nThe Durbin-Watson test is used to detect autocorrelation in the residuals. Autocorrelation occurs when the residuals are correlated with their own lagged values. The Durbin-Watson test statistic ranges from 0 to 4, where a value around 2 indicates no autocorrelation. Values significantly below 2 indicate positive autocorrelation, and values above 2 indicate negative autocorrelation.\n\n# Durbin-Watson test for autocorrelation in residuals\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\ndw_test &lt;- dwtest(simple_lm_result)\nprint(dw_test)\n\n\n    Durbin-Watson test\n\ndata:  simple_lm_result\nDW = 2.0848, p-value = 0.6637\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nDiagnostic Plot: Residual Plot\nA residual plot helps us check for the assumption of homoscedasticity, which assumes that the variance of the residuals is constant across all levels of the independent variable. We plot the residuals against the fitted values (predicted values) and look for patterns. If the plot shows a constant spread with no apparent pattern, it indicates that the model meets the assumption.\n\n# Create a residual plot\nplot(simple_lm_result$fitted.values, simple_lm_result$residuals,\n     main = \"Residual Plot\", xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\nStatistical Test: Breusch-Pagan Test (for Homoscedasticity)\nThe Breusch-Pagan test is used to test for the assumption of homoscedasticity. The null hypothesis is that the variance of the residuals is constant (homoscedastic). A significant p-value suggests the presence of heteroscedasticity.\n\n# Breusch-Pagan test for homoscedasticity\nbp_test &lt;- bptest(simple_lm_result)\nprint(bp_test)\n\n\n    studentized Breusch-Pagan test\n\ndata:  simple_lm_result\nBP = 0.26677, df = 1, p-value = 0.6055\n\n\nRemember that diagnostic plots and statistical tests should be used as tools to assess the model’s validity and identify potential issues. It is crucial to interpret the results in conjunction with domain knowledge and the specific context of the analysis. Addressing any deviations from assumptions may involve data transformation or choosing a different model to improve the model’s performance and reliability.\nGeneralized linear model\nANOVA\nOne factor\nTwo factors\nANCOVA\nPCA\nDistance matrix and classification\nDAPC"
  },
  {
    "objectID": "R_mastery.html#graphical-visualisation",
    "href": "R_mastery.html#graphical-visualisation",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "Graphical visualisation",
    "text": "Graphical visualisation\nBase R\nggplot2"
  },
  {
    "objectID": "R_mastery.html#programming-in-r",
    "href": "R_mastery.html#programming-in-r",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "Programming in R",
    "text": "Programming in R\nConsequat elit laborum id eiusmod nisi ut minim officia. Tempor non consectetur veniam sit nisi nostrud Lorem minim quis amet anim duis. Sunt deserunt excepteur velit. Ipsum elit ullamco nostrud ad veniam et.\nDeserunt aute proident commodo nostrud. Esse veniam et duis ipsum adipisicing dolor incididunt. Labore deserunt anim commodo. Magna Lorem consectetur excepteur aliquip qui voluptate nulla velit Lorem Lorem minim nulla."
  },
  {
    "objectID": "R_mastery.html#reproducibility",
    "href": "R_mastery.html#reproducibility",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Paris\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lmtest_0.9-40   zoo_1.8-12      MASS_7.3-60     lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1  \n [7] dplyr_1.1.4     purrr_1.0.2     readr_2.1.4     tidyr_1.3.0     tibble_3.2.1    ggplot2_3.4.4  \n[13] tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.4      jsonlite_1.8.8    compiler_4.3.2    tidyselect_1.2.0  scales_1.3.0     \n [6] yaml_2.3.8        fastmap_1.1.1     lattice_0.21-9    R6_2.5.1          generics_0.1.3   \n[11] knitr_1.45        htmlwidgets_1.6.4 munsell_0.5.0     pillar_1.9.0      tzdb_0.4.0       \n[16] rlang_1.1.2       utf8_1.2.4        stringi_1.8.3     xfun_0.41         timechange_0.2.0 \n[21] cli_3.6.2         withr_2.5.2       magrittr_2.0.3    digest_0.6.33     grid_4.3.2       \n[26] rstudioapi_0.15.0 hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5       evaluate_0.23    \n[31] glue_1.6.2        fansi_1.0.6       colorspace_2.1-0  rmarkdown_2.25    tools_4.3.2      \n[36] pkgconfig_2.0.3   htmltools_0.5.7"
  },
  {
    "objectID": "R_mastery.html#note",
    "href": "R_mastery.html#note",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "Note",
    "text": "Note\nThe writing process was conducted with the help of ChatGPT (OpenAI)."
  },
  {
    "objectID": "projects/post-with-code/index.html",
    "href": "projects/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "projects/RMastery/index.html",
    "href": "projects/RMastery/index.html",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "",
    "text": "This comprehensive document contains all the information I have ever used in R, ranging from fundamental statistical analysis and basic graphical representations to more advanced visualizations using various packages and the essential principles of programming with R. Whether you are a beginner or an advanced user, this document has something for everyone.\nYou will find a wealth of information on statistical analysis, including descriptive statistics, hypothesis testing, regression analysis, and more. In addition to these basic concepts, I also cover more advanced topics such as data visualization with ggplot2, which will enable you to create complex and aesthetically pleasing graphs to better illustrate your data.\nMoreover, this document provides an overview of R programming essentials, including the data structures, functions, loops, and conditionals that are the building blocks of the language. By understanding these basic concepts, you can more easily manipulate and analyze data in R.\nTo ensure that this code can be reproduced if needed, I have included a chapter on “Reproducibility” at the end of this document. In this section, I provide a summary of every package I used, as well as their version, which can be useful if the code breaks. Overall, this document is a valuable resource for anyone who wants to improve their R programming skills and gain a deeper understanding of statistical analysis and data visualization."
  },
  {
    "objectID": "projects/Yersinia/index.html",
    "href": "projects/Yersinia/index.html",
    "title": "Yersinia pestis dashboard",
    "section": "",
    "text": "To access the dashboard, simply click on this link.\nDetailed descriptions of the bioinformatic analyses can be found in the following tutorial."
  },
  {
    "objectID": "projects/BloodGroups/index.html",
    "href": "projects/BloodGroups/index.html",
    "title": "Poster: “Blood groups systems polymorphism in Papua New-Guinea”",
    "section": "",
    "text": "Poster"
  },
  {
    "objectID": "projects/RMastery/index.html#overview",
    "href": "projects/RMastery/index.html#overview",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "",
    "text": "This comprehensive document contains all the information I have ever used in R, ranging from fundamental statistical analysis and basic graphical representations to more advanced visualizations using various packages and the essential principles of programming with R. Whether you are a beginner or an advanced user, this document has something for everyone.\nYou will find a wealth of information on statistical analysis, including descriptive statistics, hypothesis testing, regression analysis, and more. In addition to these basic concepts, I also cover more advanced topics such as data visualization with ggplot2, which will enable you to create complex and aesthetically pleasing graphs to better illustrate your data.\nMoreover, this document provides an overview of R programming essentials, including the data structures, functions, loops, and conditionals that are the building blocks of the language. By understanding these basic concepts, you can more easily manipulate and analyze data in R.\nTo ensure that this code can be reproduced if needed, I have included a chapter on “Reproducibility” at the end of this document. In this section, I provide a summary of every package I used, as well as their version, which can be useful if the code breaks. Overall, this document is a valuable resource for anyone who wants to improve their R programming skills and gain a deeper understanding of statistical analysis and data visualization."
  },
  {
    "objectID": "projects/RMastery/index.html#data-manipulation",
    "href": "projects/RMastery/index.html#data-manipulation",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "Data manipulation",
    "text": "Data manipulation\nTo begin with R it is primordial to know your way around a data frame, in this section I will go over the functions needed to shape your date like you need it.\nBase R\nThe most basic manipulation and overview functions are directly in R without any package necessary.\nImport your data\nThe first step is to import your data. To import data into R, there are several functions you can use, including read.table(), read.csv(), read.csv2(), read.delim(), read.delim2(), and more. Here’s a brief explanation of each function:\n\n\nread.table(): This function is used to read data from a text file. It is a very versatile function that can handle many different file formats, but it requires you to specify some arguments such as the file path, the delimiter used in the file, and if the file contains a header or not.\n\nread.csv(): This function is similar to read.table(), but it is specifically designed to read comma-separated value (CSV) files. It assumes that the file has a header row, and it automatically sets the delimiter to a comma.\n\nread.csv2(): This function is similar to read.csv(), but it is specifically designed to read CSV files where the decimal separator is a semicolon (;) and the field separator is a comma (,).\n\nread.delim(): This function is similar to read.csv(), but it is specifically designed to read tab-delimited files.\n\nread.delim2(): This function is similar to read.delim(), but it is specifically designed to read tab-delimited files where the decimal separator is a semicolon (;) and the field separator is a tab (\\t). For example, to read a CSV file named “data.csv” in your current working directory, you can use the read.csv function as follows:\n\n\ndata &lt;- read.csv(\"data.csv\")\n\nThis assumes that the file has a header row, uses a comma as the delimiter, and that the file is located in your current working directory.\nYou can adjust the parameters of these functions based on the specifics of your data and the file format. For more information, you can use the R help system by typing ?read.csv, ?read.table, etc.\nWhat does your data look like?\nIn this section I will give you some of the most commonly used functions in R for exploring and summarizing data.\nThe function head() is used to display the first few rows of a data frame. By default, it displays the first six rows, but you can specify the number of rows you want to display.\n\ndata(iris) # load the iris dataset\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nThe function summary() is used to generate a summary of a data frame, including measures of central tendency and variability for each variable. It is particularly useful for identifying missing values, outliers, and the distribution of the data.\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width          Species  \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100   setosa    :50  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300   versicolor:50  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300   virginica :50  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199                  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800                  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500                  \n\n\nThe str() function is used to display the structure of a data frame, including the number of observations and variables, their names, data types, and a preview of the data. It is particularly useful for identifying the format of the data and for checking if variables are of the right data type.\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nThe colSums(is.na(data)) function is used to check for missing values in a data frame. It returns the total number of missing values per column. It is particularly useful for identifying if any variables have missing data and for imputing missing values.\n\ncolSums(is.na(iris))\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n           0            0            0            0            0 \n\n\nBasic functions\nTo effectively modify and customize your data frames, you’ll need to utilize a variety of functions available in R. Fortunately, there are numerous options available to help you achieve your desired results. Below are some examples of functions you may find helpful in your data frame manipulations.\nOne of the most useful function is subset(). This function allows you to subset a data frame based on one or more conditions. Here’s an example of using subset() to select only the rows of the iris dataframe where the value in the Sepal.Length column is greater than 5:\n\niris_subset &lt;- subset(iris, Sepal.Length &gt; 5)\nhead(iris_subset)\n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1           5.1         3.5          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n11          5.4         3.7          1.5         0.2  setosa\n15          5.8         4.0          1.2         0.2  setosa\n16          5.7         4.4          1.5         0.4  setosa\n17          5.4         3.9          1.3         0.4  setosa\n\n\nThe function merge() allows you to merge two or more data frames based on a common variable. Here’s an example of using merge() to combine two data frames based on the common ID column:\n\n# create two sample data frames\ndf1 &lt;- data.frame(id = c(1, 2, 3), x = c(4, 5, 6))\ndf2 &lt;- data.frame(id = c(2, 3, 4), y = c(7, 8, 9))\n\n# use merge to combine the data frames based on the \"id\" column\nmerged_df &lt;- merge(df1, df2, by = \"id\")\nmerged_df\n\n  id x y\n1  2 5 7\n2  3 6 8\n\n\nThe function apply() can be used to apply a function to rows or columns of a data frame. In this example, the code applies the sum function to each row of the data frame, if you wanted to sum each column you can change (1) to (2).\n\n# create a sample data frame\ndf &lt;- data.frame(x = c(1, 2, 3), \n                 y = c(4, 5, 6), \n                 z = c(7, 8, 9))\n\n# use apply to calculate the row sums\nrow_sums &lt;- apply(df, 1, sum)\nrow_sums\n\n[1] 12 15 18\n\n\nThe tapply() function can be used to apply a function to subsets of a data frame defined by one or more variables. Here’s an example of using tapply() to calculate the mean of a variable for each level of a grouping variable.\n\n# create a sample data frame\ndf &lt;- data.frame(x = c(1, 2, 3, 4), \n                 y = c(5, 6, 7, 8), \n                 group = c(\"A\", \"A\", \"B\", \"B\"))\n\n# use tapply to calculate the mean of y for each group\nmean_by_group &lt;- tapply(df$y, df$group, mean)\nmean_by_group\n\n  A   B \n5.5 7.5 \n\n\nAnother function from the apply environment is lapply(). This function can be used to apply a function to each column of a data frame and return a list of the results. Here’s an example of using lapply() to calculate the range of values for each column of a data frame:\n\n# create a sample data frame\ndf &lt;- data.frame(x = c(1, 2, 3), \n                 y = c(4, 5, 6), \n                 z = c(7, 8, 9))\n\n# use lapply to calculate the range of values for each column\nrange_by_col &lt;- lapply(df, range)\nrange_by_col\n\n$x\n[1] 1 3\n\n$y\n[1] 4 6\n\n$z\n[1] 7 9\n\n\nIf you want to simply the output to a vector (or matrix if possible) you can use the sapply() function. In this example we calculate the mean of each column of a data frame:\n\n# create a sample data frame\ndf &lt;- data.frame(x = c(1, 2, 3), \n                 y = c(4, 5, 6), \n                 z = c(7, 8, 9))\n\n# use sapply to calculate the mean of each column\nmean_by_col &lt;- sapply(df, mean)\nmean_by_col\n\nx y z \n2 5 8 \n\n\nIf you want to change the column names, you can use the colnames() function like in this example:\n\n# Create a sample data frame\ndf &lt;- data.frame(x = c(1, 2, 3), \n                 y = c(4, 5, 6))\n\n# Set column names to be \"Var1\" and \"Var2\"\ncolnames(df) &lt;- c(\"Var1\", \"Var2\")\n\n# Print the data frame with column names changed\ndf\n\n  Var1 Var2\n1    1    4\n2    2    5\n3    3    6\n\n\nYou can also change the row names with the rownames() function as displayed here:\n\n# Create a sample data frame\ndf &lt;- data.frame(x = c(1, 2, 3), \n                 y = c(4, 5, 6))\n\n# Set row names to be \"A\", \"B\", and \"C\"\nrow.names(df) &lt;- c(\"A\", \"B\", \"C\")\n\n# Print the data frame with row names changed\ndf\n\n  x y\nA 1 4\nB 2 5\nC 3 6\n\n\nMoving to the tidyverse\nThe tidyverse is a collection of R packages that are designed to work together to make data analysis more efficient and intuitive. The packages in the tidyverse include:\n\n\nggplot2 for creating visualizations\n\ndplyr for data manipulation\n\ntidyr for reshaping data\n\nreadr for importing data\n\npurrr for functional programming\n\ntibble for creating data frames\n\n\nlibrary(tidyverse)\n\nIn the tidyverse something very useful is the pipe operator: %&gt;%, it allows you to chain together multiple functions in a way that makes the code easier to read and understand. It works by taking the output of one function and passing it as the first argument to the next function in the chain. This eliminates the need to create intermediate variables to store the output of each function, and makes it easier to write code that is both concise and expressive.\nThe readr package\nThis package provides a fast and friendly way to read in flat files, including CSV, TSV, and fixed-width files. It is designed to be faster and more memory-efficient than the base R functions for reading in data.\nThe package provides several functions for reading in data, including:\n\n\nread_csv(): for reading in CSV files\n\nread_tsv(): for reading in TSV files\n\nread_delim(): for reading in files with a delimiter of your choice\n\nread_fwf(): for reading in fixed-width files\n\nThese functions are designed to be fast and memory-efficient, and they also automatically detect and handle a wide range of data formats and encodings.\nOne of the key features of readr is its ability to automatically detect the types of columns in your data, such as numeric, character, or date/time columns. However, if you have specific requirements for the types of columns in your data, you can also specify the column types manually using the col_types argument.\nFor example, if you want to specify that the second column in your data is a character column and the fourth column is a numeric column, you can do:\n\nmy_data &lt;- read_csv(\"my_data.csv\", col_types = cols(\n  col2 = col_character(),\n  col4 = col_double()\n))\n\nreadr provides several options for handling missing values in your data. By default, readr will treat any value that is empty or matches the string “NA” as a missing value.\nYou can also specify additional strings to be treated as missing values using the na argument. For example, if you want to treat the string “MISSING” as a missing value, you can do:\n\nmy_data &lt;- read_csv(\"my_data.csv\", na = c(\"NA\", \"MISSING\"))\n\nThe dplyr package\ndplyr is a powerful and widely-used package in R for data manipulation. It provides a concise and consistent syntax for transforming data sets, which allows you to easily filter, arrange, summarize, and join data.\nHere’s a brief overview of the main functions in dplyr:\n\n\nselect(): Allows you to select specific columns from a data set, using a simple syntax that includes the column names separated by commas.\n\nfilter(): Allows you to select specific rows from a data set based on a set of conditions, using a simple syntax that includes logical operators such as ==, &gt;, &lt;, etc.\n\narrange(): Allows you to sort a data set based on one or more columns, using a simple syntax that includes the column names separated by commas, with optional modifiers like desc() for descending order.\n\nmutate(): Allows you to create new columns in a data set based on calculations or transformations of existing columns, using a simple syntax that includes the name of the new column followed by an equals sign (=) and the calculation or transformation expression.\n\nsummarize(): Allows you to aggregate data by computing summary statistics such as mean, median, count, etc., using a simple syntax that includes the summary function name followed by the name of the column to be summarized.\n\ngroup_by(): Allows you to group a data set by one or more columns, which is often used in conjunction with summarize() to compute summary statistics for each group.\n\njoin(): Allows you to combine two or more data sets based on common columns, using a simple syntax that includes the names of the data sets and the columns to be joined on.\n\nHere’s an example of how to use dplyr to perform some common data manipulation tasks:\n\nlibrary(dplyr)\n\n# load a sample data set\nmy_data &lt;- iris\n\n# select specific columns\nmy_data &lt;- select(my_data, Sepal.Length, Sepal.Width, Species)\n\n# filter rows based on a condition\nmy_data &lt;- filter(my_data, Sepal.Length &gt; 5)\n\n# arrange data by a column\nmy_data &lt;- arrange(my_data, Sepal.Length)\n\n# create a new column based on an existing one\nmy_data &lt;- mutate(my_data, Sepal.Area = Sepal.Length * Sepal.Width)\n\n# summarize data by a column\nmy_summary &lt;- summarise(my_data, Mean.Sepal.Length = mean(Sepal.Length))\n\n# group data by a column and summarize\nmy_summary2 &lt;- my_data %&gt;%\n  group_by(Species) %&gt;%\n  summarise(Mean.Sepal.Length = mean(Sepal.Length))\n\n# join two data sets\nmy_data2 &lt;- data.frame(Species = c(\"setosa\", \"versicolor\", \"virginica\"),\n                       Petal.Length = c(1.4, 4.2, 6.0))\nmy_data &lt;- left_join(my_data, my_data2, by = \"Species\")\n\nIn this example, we load the iris data set, which contains information about flowers, and use various dplyr functions to select specific columns, filter rows based on a condition, arrange the data by a column, create a new column based on an existing one, summarize the data by a column, group the data by a column and summarize, and join two data sets based on a common column.\nOverall, dplyr provides a powerful and flexible set of tools for data manipulation in R, and is a key package to have in your data science toolkit.\nThe tidyr package\nThe tidyr package is a data manipulation package in R that provides functions for reshaping and tidying data sets. It is a complementary package to dplyr, which focuses on manipulating data sets by filtering, summarizing, and transforming variables.\nThe gather() function is used to reshape a data frame from a wide format to a long format. It does this by taking multiple columns and “gathering” them into a key-value pair format. For example, if you have a data frame with columns for year, quarter, and sales for each quarter, you can use gather() to reshape it into a format where each row represents a year-quarter combination and the sales value is in a single column.\n\n# create example data frame\ndf &lt;- data.frame(year = c(2018, 2019),\n                 q1_sales = c(100, 200),\n                 q2_sales = c(150, 250),\n                 q3_sales = c(175, 300),\n                 q4_sales = c(200, 350))\n\n# reshape data frame\ndf2 &lt;- gather(df, key = \"quarter\", value = \"sales\", -year)\n\nThe function spread() is the opposite of gather(). It is used to reshape a data frame from a long format to a wide format. It does this by taking a key-value pair and “spreading” the values across multiple columns. For example, if you have a data frame with columns for year, quarter, and sales, where each row represents a year-quarter combination, you can use spread() to reshape it into a format where each row represents a year and there is a column for each quarter’s sales value.\n\n# create example data frame\ndf &lt;- data.frame(month = c(\"Jan\", \"Feb\", \"Mar\", \"Jan\", \"Feb\", \"Mar\"),\n                 year = c(2018, 2018, 2018, 2019, 2019, 2019),\n                 rainfall = c(50, 75, 100, 60, 80, 110))\n\n# reshape data frame\ndf2 &lt;- spread(df, key = \"month\", value = \"rainfall\")\n\nThe separate() function is used to split a single column into multiple columns based on a delimiter or a fixed number of characters. For example, if you have a data frame with a column for name that contains both first and last names, you can use separate() to split the column into two columns.\n\n# create example data frame\ndf &lt;- data.frame(name = c(\"John Smith\", \"Jane Doe\", \"Bob Johnson\"))\n\n# separate name column into first and last name columns\ndf2 &lt;- separate(df, col = name, into = c(\"first_name\", \"last_name\"), sep = \" \")\n\nThe function unite() is the opposite of separate(). It is used to combine multiple columns into a single column by concatenating them with a delimiter. For example, if you have a data frame with separate columns for first and last names, you can use unite() to combine them into a single column.\n\n# create example data frame\ndf &lt;- data.frame(first_name = c(\"John\", \"Jane\", \"Bob\"),\n                 last_name = c(\"Smith\", \"Doe\", \"Johnson\"))\n\n# combine first and last name columns into a single name column\ndf2 &lt;- unite(df, col = \"name\", first_name, last_name, sep = \" \")\n\nThe fill() function is used to fill in missing values in a data frame. It does this by filling in missing values with the most recent non-missing value in the same column. For example, if you have a data frame with a column for price that has missing values, you can use fill() to fill in those missing values with the most recent non-missing price value.\n\n# create example data frame\ndf &lt;- data.frame(date = c(\"2022-01-01\", \"2022-01-02\", \"2022-01-03\", \"2022-01-04\"),\n                 price = c(100, NA, 150, NA))\n\n# fill in missing values with most recent non-missing value\ndf2 &lt;- fill(df, price)\ndf2\n\n        date price\n1 2022-01-01   100\n2 2022-01-02   100\n3 2022-01-03   150\n4 2022-01-04   150\n\n\ndrop_na() is a useful function when we have a data frame with missing values, and we want to remove any rows that contain missing values.\n\n# create example data frame\ndf &lt;- data.frame(id = 1:5,\n                 name = c(\"John\", NA, \"Bob\", \"Jane\", \"Bill\"),\n                 age = c(30, 25, NA, 40, 35))\n\n# drop any rows that contain missing values\ndf2 &lt;- drop_na(df)\n\nOverall, tidyr is a powerful package that provides a range of functions for tidying and reshaping data frames. It is a key component in the “tidyverse” of packages in R that are designed to work together to provide a consistent and efficient data analysis workflow.\nThe tibble package\nThe tibble package is an alternative to R’s built-in data.frame object. It is designed to be more user-friendly and to make working with data frames easier. Some of the key features of tibble include:\n\nPrinting: tibble objects print nicely to the console, making them easier to read.\nError messages: tibble objects have improved error messages that provide more context than the default data.frame error messages.\nSubsetting: tibble objects behave more consistently than data.frame objects when subsetting.\nColumn types: tibble objects preserve column types better than data.frame objects.\n\nHere is an example of how to create and work with a tibble object:\n\n# create a tibble with three columns\nmy_data &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  has_pet = c(TRUE, FALSE, TRUE)\n)\n\n# print the tibble to the console\nmy_data\n\n# A tibble: 3 × 3\n  name      age has_pet\n  &lt;chr&gt;   &lt;dbl&gt; &lt;lgl&gt;  \n1 Alice      25 TRUE   \n2 Bob        30 FALSE  \n3 Charlie    35 TRUE   \n\n# filter the tibble to only include people with pets\nmy_data_with_pets &lt;- filter(my_data, has_pet == TRUE)\n\n# add a new column to the tibble\nmy_data_with_pets &lt;- mutate(my_data_with_pets, pet_type = c(\"dog\", \"cat\"))\n\n# print the modified tibble to the console\nmy_data_with_pets\n\n# A tibble: 2 × 4\n  name      age has_pet pet_type\n  &lt;chr&gt;   &lt;dbl&gt; &lt;lgl&gt;   &lt;chr&gt;   \n1 Alice      25 TRUE    dog     \n2 Charlie    35 TRUE    cat     \n\n\nIn this example, we create a tibble object called my_data with three columns: name, age, and has_pet. We then print the tibble to the console, which shows the nicely formatted output. We use filter() to create a new tibble called my_data_with_pets that only includes rows where has_pet is TRUE. We then use mutate() to add a new column called pet_type to my_data_with_pets. Finally, we print the modified tibble to the console, which shows the added pet_type column.\nOther packages\nggplot2 is an R package for data visualization that is based on the grammar of graphics, which provides a consistent framework for building graphics by breaking them down into components. The package allows for a high degree of flexibility and customization, allowing users to create a wide range of visualizations, from simple scatterplots to complex multi-layered graphics. ggplot2 works by mapping data to aesthetic properties of the plot, such as color, shape, and size, and then layering geometric objects, such as points, lines, and bars, on top of this mapping. This allows for easy manipulation and comparison of different aspects of the data. The package also provides many tools for modifying and fine-tuning the appearance of the plot, such as changing axis labels and limits, adjusting colors and themes, and adding annotations. Overall, ggplot2 is a powerful and flexible tool for creating high-quality visualizations in R. We will dive deeper in ggplot2 in a future chapter.\nThe other tydiverse package not aforementioned is the purrr package. It is a powerful and flexible package for working with functions and vectors in R. It provides a consistent set of tools for manipulating and transforming data in a functional programming style. Some of the key features of purrr include the ability to work with functions in a more flexible way, including functions with multiple arguments and complex inputs, as well as the ability to apply functions to data in a variety of ways, such as mapping over lists and data frames. purrr also provides many functions for working with vectors, including tools for filtering, grouping, and reshaping data, as well as for handling missing values and dealing with errors. Overall, purrr is a powerful and flexible package that provides a consistent and functional approach to working with data in R."
  },
  {
    "objectID": "projects/RMastery/index.html#statistical-analysis",
    "href": "projects/RMastery/index.html#statistical-analysis",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "Statistical analysis",
    "text": "Statistical analysis\nIn this chapter we will focus on statistical tests, how to use them but also interpret them.\nTest for normality and transformations\nIn statistical analysis, it is often essential to check whether a dataset follows a normal distribution before applying certain parametric tests. R provides various tools to assess the normality of data and apply transformations if necessary. In this section, we will explore some common methods to test for normality and demonstrate how to perform data transformations.\nShapiro-Wilk Test\nThe Shapiro-Wilk test is a popular method to assess normality. It tests the null hypothesis that a sample is drawn from a normally distributed population. We can use the shapiro.test() function in R to perform this test. Let’s see an example:\n\n# Generate a random sample from a normal distribution\nset.seed(123)\nnormal_sample &lt;- rnorm(100)\n\n# Perform Shapiro-Wilk test\nshapiro.test(normal_sample)\n\n\n    Shapiro-Wilk normality test\n\ndata:  normal_sample\nW = 0.99388, p-value = 0.9349\n\n\nHistogram and Q-Q Plot\nAnother visual approach to check for normality is by creating a histogram and a quantile-quantile (Q-Q) plot of the data. The histogram provides a rough representation of the data’s distribution, while the Q-Q plot compares the quantiles of the data to the quantiles of a theoretical normal distribution. If the points on the Q-Q plot fall approximately along a straight line, it suggests the data is normally distributed. Here’s an example:\n\n# Generate a random sample from a non-normal distribution\nset.seed(456)\nnon_normal_sample &lt;- rgamma(100, shape = 2, rate = 1)\n\n# Create a histogram\nhist(non_normal_sample, main = \"Histogram of Non-Normal Data\")\n\n\n\n\n\n\n# Create a Q-Q plot\nqqnorm(non_normal_sample)\nqqline(non_normal_sample, col = \"red\")\n\n\n\n\n\n\n\nData Transformation\nIf the data significantly deviates from normality, we can try applying transformations to make it approximately normal. Common transformations include logarithmic, square root, and reciprocal transformations. Let’s apply a logarithmic transformation to our non-normal data and re-check for normality:\n\n# Apply logarithmic transformation\nlog_transformed_data &lt;- log(non_normal_sample)\n\n# Create a histogram of transformed data\nhist(log_transformed_data, main = \"Histogram of Log-Transformed Data\")\n\n\n\n\n\n\n# Create a Q-Q plot of transformed data\nqqnorm(log_transformed_data)\nqqline(log_transformed_data, col = \"red\")\n\n\n\n\n\n\n# Perform Shapiro-Wilk test on transformed data\nshapiro.test(log_transformed_data)\n\n\n    Shapiro-Wilk normality test\n\ndata:  log_transformed_data\nW = 0.95742, p-value = 0.002644\n\n\nRemember that data transformations should be applied judiciously and be interpreted cautiously in the context of your specific analysis.\nIn conclusion, R offers powerful tools to assess the normality of data and apply appropriate transformations to meet the assumptions of parametric tests. Always validate the results and consider the specific requirements of your analysis before making any conclusions based on the normality tests and transformations.\nThe Chi-square test of independence\nThe Chi-square test of independence is a fundamental statistical method used to determine whether there is a significant association between two categorical variables. It is applicable when we have two or more categorical variables, and we want to investigate if they are dependent or independent of each other. The test assesses whether the observed frequencies in a contingency table differ significantly from the frequencies we would expect under the assumption of independence between the variables.\nLet’s go through an example using R to illustrate the Chi-square test of independence.\nSuppose we have survey data on the preference for ice cream flavors among two groups of people: Group A and Group B. The data is organized in a contingency table:\n\n\n\nVanilla\nChocolate\nStrawberry\n\n\n\nGroup A\n25\n15\n10\n\n\nGroup B\n30\n20\n25\n\n\n\nWe can use the chisq.test() function in R to perform the Chi-square test:\n\n# Create the contingency table\nice_cream_table &lt;- matrix(c(25, 15, 10, 30, 20, 25), nrow = 2, byrow = TRUE,\n                          dimnames = list(c(\"Group A\", \"Group B\"),\n                                          c(\"Vanilla\", \"Chocolate\", \"Strawberry\")))\n\n# Perform the Chi-square test\nchi_sq_result &lt;- chisq.test(ice_cream_table)\nprint(chi_sq_result)\n\n\n    Pearson's Chi-squared test\n\ndata:  ice_cream_table\nX-squared = 2.7056, df = 2, p-value = 0.2585\n\n\nThe test will provide us with a chi-square statistic and a p-value. If the p-value is below a predetermined significance level (often set at 0.05), we reject the null hypothesis of independence and conclude that there is a significant association between the two categorical variables. Conversely, if the p-value is greater than 0.05, we fail to reject the null hypothesis, suggesting that there is no significant association between the variables.\nIn summary, the Chi-square test of independence is a valuable tool to assess the relationship between categorical variables. It is commonly used in various fields, including social sciences, medicine, marketing, and more, to gain insights into the associations between different groups and make informed decisions based on the observed data.\nCorrelation tests\nCorrelation tests are essential statistical techniques used to assess the strength and direction of the relationship between two continuous variables. In R, we can employ various correlation tests to determine if there is a significant linear association between the variables. Two commonly used correlation tests are the Pearson correlation coefficient and the Spearman rank correlation coefficient.\nPearson Correlation Coefficient\nThe Pearson correlation coefficient, also known as Pearson’s r, measures the linear relationship between two continuous variables. It ranges from -1 to 1, where -1 indicates a perfect negative linear relationship, 1 indicates a perfect positive linear relationship, and 0 suggests no linear relationship. The cor() function in R can be used to compute Pearson’s correlation coefficient. Here’s an example:\n\n# Generate sample data\nset.seed(789)\nvariable1 &lt;- rnorm(100)\nvariable2 &lt;- rnorm(100)\n\n# Calculate Pearson correlation\ncor(variable1, variable2, method = \"pearson\")\n\n[1] -0.05680459\n\n\nSpearman Rank Correlation Coefficient\nThe Spearman rank correlation coefficient assesses the strength and direction of a monotonic relationship between two variables. It is suitable for variables that may not have a linear relationship but exhibit a consistent monotonic trend. The cor() function with method = \"spearman\" can be used to compute Spearman’s correlation coefficient. Here’s an example:\n\n# Generate sample data\nset.seed(987)\nvariable3 &lt;- runif(100)\nvariable4 &lt;- runif(100)\n\n# Calculate Spearman correlation\ncor(variable3, variable4, method = \"spearman\")\n\n[1] -0.1977798\n\n\nIn both cases, the correlation tests will produce a correlation coefficient and a p-value. The p-value indicates the statistical significance of the correlation. A p-value less than the chosen significance level (often 0.05) suggests that the correlation is statistically significant.\nUnderstanding the relationship between variables is crucial in data analysis and can provide valuable insights for decision-making and model building. Correlation tests help us quantify the association between variables, enabling us to make informed conclusions about their interdependence in the dataset.\nLinear regression\nLinear regression is a powerful statistical technique used to model the relationship between a dependent variable and one or more independent variables. It is widely employed in various fields, including economics, social sciences, and machine learning. In R, we can use the lm() function to perform linear regression and estimate the coefficients of the regression equation. Let’s explore three aspects of linear regression: simple linear regression, multiple linear regression, and variable selection with model comparison.\nSimple linear regression\nSimple linear regression involves modeling the relationship between two continuous variables: a dependent variable and a single independent variable. It assumes that the relationship can be approximated by a straight line equation (y = mx + b), where y represents the dependent variable, x is the independent variable, m is the slope, and b is the intercept. We can use the lm() function to perform simple linear regression in R. Here’s an example:\n\n# Generate sample data\nset.seed(456)\nindependent_var &lt;- rnorm(100)\ndependent_var &lt;- 2 * independent_var + rnorm(100)\n\n# Perform simple linear regression\nsimple_lm_result &lt;- lm(dependent_var ~ independent_var)\nsummary(simple_lm_result)\n\n\nCall:\nlm(formula = dependent_var ~ independent_var)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.21548 -0.81266 -0.04914  0.91531  1.91311 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     -0.11472    0.09834  -1.166    0.246    \nindependent_var  1.98347    0.09797  20.246   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9763 on 98 degrees of freedom\nMultiple R-squared:  0.807, Adjusted R-squared:  0.8051 \nF-statistic: 409.9 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nThe output of the summary() function will provide information about the following key components:\n\nCoefficients Table:\nThe coefficients table shows the estimated regression coefficients for the intercept ((Intercept)) and the independent variable (independent_var). The coefficient estimates represent the change in the dependent variable (dependent_var) for a one-unit increase in the independent variable while holding other variables constant.\nStandard Error:\nThe standard error is an estimate of the variability or uncertainty associated with the coefficient estimates. Smaller standard errors indicate more precise estimates.\nt-value:\nThe t-value measures how many standard errors the coefficient estimate is away from zero. It is used to test the null hypothesis that the true coefficient is zero (i.e., no relationship between the independent and dependent variables). A high t-value indicates a significant relationship between the variables.\nPr(&gt;|t|):\nThe p-value associated with the t-value. It indicates the probability of observing the t-value or more extreme values under the null hypothesis. A p-value below the chosen significance level (often 0.05) suggests a statistically significant relationship.\nResidual Standard Error (RSE):\nThe residual standard error is an estimate of the standard deviation of the residuals (the differences between the observed and predicted values). It measures the goodness of fit of the model - smaller RSE indicates a better fit.\nR-squared (R2):\nThe R-squared value represents the proportion of variance in the dependent variable explained by the independent variable(s). It ranges from 0 to 1, with higher values indicating a better fit. For simple linear regression, R-squared is equal to the square of the correlation coefficient between the dependent and independent variables.\nMultiple linear regression\nMultiple linear regression extends the concept of simple linear regression to include multiple independent variables. It models the relationship between a dependent variable and two or more independent variables, assuming a linear combination of these variables to predict the dependent variable. The lm() function can also be used for multiple linear regression. Let’s see an example:\n\n# Generate sample data\nset.seed(789)\nindependent_var1 &lt;- rnorm(100)\nindependent_var2 &lt;- rnorm(100)\ndependent_var_multiple &lt;- 2 * independent_var1 + 3 * independent_var2 + rnorm(100)\n\n# Perform multiple linear regression\nmultiple_lm_result &lt;- lm(dependent_var_multiple ~ independent_var1 + independent_var2)\nsummary(multiple_lm_result)\n\n\nCall:\nlm(formula = dependent_var_multiple ~ independent_var1 + independent_var2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1398 -0.6257  0.1290  0.6580  2.2478 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -0.09354    0.09893  -0.946    0.347    \nindependent_var1  2.08573    0.10162  20.524   &lt;2e-16 ***\nindependent_var2  2.95072    0.09538  30.937   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.986 on 97 degrees of freedom\nMultiple R-squared:  0.9311,    Adjusted R-squared:  0.9297 \nF-statistic: 655.2 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nThe summary() output provides coefficient estimates for each independent variable, along with their standard errors, t-values, and p-values. Additionally, it gives an overall assessment of the model’s fit.\nThe summary table for multiple linear regression is similar to the one for simple linear regression. However, in multiple linear regression, the coefficients table will contain estimates for each independent variable, along with the intercept.\nAdditionally, you will find the following:\n\nMultiple R-squared:\nThe multiple R-squared (R2) indicates the proportion of variance in the dependent variable explained by all the independent variables in the model.\nAdjusted R-squared:\nThe adjusted R-squared adjusts the multiple R-squared value based on the number of independent variables and the sample size. It penalizes the addition of unnecessary variables to prevent overfitting. A higher adjusted R-squared suggests a better-fitting model.\n\nThe summary() output also includes information on residuals and various diagnostics that can be useful for evaluating the model’s assumptions and checking for potential issues like heteroscedasticity or influential outliers.\nRemember that while the summary table provides useful information about the model, it is essential to consider other diagnostic plots and statistical tests to ensure the model’s validity and interpret the results properly.\nVariable selection and model comparison\nIn linear regression, selecting the most relevant independent variables is essential to build a robust and interpretable model. There are various approaches to perform variable selection, such as forward selection, backward elimination, and stepwise regression. Additionally, model comparison helps us evaluate different models and choose the best-fit model based on several metrics, including R-squared, adjusted R-squared, AIC (Akaike Information Criterion), and BIC (Bayesian Information Criterion).\nLet’s consider a dataset with multiple potential independent variables. We can use different variable selection techniques to identify the subset of variables that contribute most significantly to the model. In this example, we’ll use the step() function in R to perform a stepwise regression for variable selection:\n\n# Load necessary library for stepwise regression\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n# Generate sample data\nset.seed(123)\nindependent_var1 &lt;- rnorm(100)\nindependent_var2 &lt;- rnorm(100)\ndependent_var &lt;- 3 * independent_var1 + 2 * independent_var2 + rnorm(100)\n\n# Perform stepwise regression for variable selection\nmodel &lt;- lm(dependent_var ~ independent_var1 + independent_var2)\nselected_model &lt;- step(model)\n\nStart:  AIC=-7.03\ndependent_var ~ independent_var1 + independent_var2\n\n                   Df Sum of Sq    RSS     AIC\n&lt;none&gt;                           87.78  -7.032\n- independent_var2  1    378.22 466.01 157.903\n- independent_var1  1    676.30 764.08 207.350\n\nsummary(selected_model)\n\n\nCall:\nlm(formula = dependent_var ~ independent_var1 + independent_var2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8730 -0.6607 -0.1245  0.6214  2.0798 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.13507    0.09614   1.405    0.163    \nindependent_var1  2.86683    0.10487  27.337   &lt;2e-16 ***\nindependent_var2  2.02381    0.09899  20.444   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9513 on 97 degrees of freedom\nMultiple R-squared:  0.9198,    Adjusted R-squared:  0.9182 \nF-statistic: 556.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nThe step() function will perform a stepwise selection and provide the final selected model with the most significant variables.\nOnce we have multiple candidate models, we need to evaluate their performance to choose the best one. Several metrics can be used for model comparison:\nR-squared (R^2)\nR-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model. A higher R-squared value indicates a better fit of the model to the data.\n\n# Calculate R-squared for the selected model\nrsquared &lt;- summary(selected_model)$r.squared\nprint(paste(\"R-squared:\", round(rsquared, 3)))\n\n[1] \"R-squared: 0.92\"\n\n\nAdjusted R-squared (R^2_adj)\nAdjusted R-squared is a modified version of R-squared that considers the number of independent variables in the model. It penalizes the inclusion of irrelevant variables, providing a more realistic assessment of model fit.\n\n# Calculate Adjusted R-squared for the selected model\nrsquared_adj &lt;- summary(selected_model)$adj.r.squared\nprint(paste(\"Adjusted R-squared:\", round(rsquared_adj, 3)))\n\n[1] \"Adjusted R-squared: 0.918\"\n\n\nAkaike Information Criterion (AIC) and Bayesian Information Criterion (BIC)\nAIC and BIC are information criteria that balance the goodness of fit with model complexity. Lower AIC and BIC values indicate a better model fit with fewer variables, making them useful for model comparison.\n\naic &lt;- AIC(selected_model)\nprint(paste(\"AIC:\", round(aic, 3)))\n\n[1] \"AIC: 278.756\"\n\nbic &lt;- BIC(selected_model)\nprint(paste(\"BIC:\", round(bic, 3)))\n\n[1] \"BIC: 289.177\"\n\n\nBy comparing these metrics across different candidate models, we can select the model that provides the best trade-off between goodness of fit and simplicity, ensuring a reliable representation of the relationship between the variables.\nIn conclusion, variable selection and model comparison are critical steps in building an effective linear regression model. Properly selected models, based on meaningful metrics such as R-squared, adjusted R-squared, AIC, and BIC, can lead to better insights and more accurate predictions. Always interpret the results with caution and consider the context and assumptions of the analysis.\nValidity of the model\nLinear regression makes several assumptions about the data, such as :\n\nLinearity of the data. The relationship between the predictor (x) and the outcome (y) is assumed to be linear.\nNormality of residuals. The residual errors are assumed to be normally distributed.\nHomogeneity of residuals variance. The residuals are assumed to have a constant variance (homoscedasticity)\nIndependence of residuals error terms.\n\nDiagnostic Plot: Q-Q Plot\nThe Q-Q (quantile-quantile) plot helps us assess the assumption of normality in the residuals. If the residuals follow a normal distribution, the points on the Q-Q plot should approximately fall along a straight line.\n\n# Create a Q-Q plot of residuals\nqqnorm(simple_lm_result$residuals, main = \"Q-Q Plot of Residuals\")\nqqline(simple_lm_result$residuals, col = \"red\")\n\n\n\n\n\n\n\nStatistical Test: Shapiro-Wilk Test\nTo complement the Q-Q plot, we can perform the Shapiro-Wilk test to formally test for the normality of the residuals. The null hypothesis is that the residuals are normally distributed. A significant p-value (p &lt; 0.05) indicates a departure from normality.\n\n# Shapiro-Wilk test for normality of residuals\nshapiro_test &lt;- shapiro.test(simple_lm_result$residuals)\nprint(shapiro_test)\n\n\n    Shapiro-Wilk normality test\n\ndata:  simple_lm_result$residuals\nW = 0.96982, p-value = 0.0214\n\n\nStatistical Test: Durbin-Watson Test\nThe Durbin-Watson test is used to detect autocorrelation in the residuals. Autocorrelation occurs when the residuals are correlated with their own lagged values. The Durbin-Watson test statistic ranges from 0 to 4, where a value around 2 indicates no autocorrelation. Values significantly below 2 indicate positive autocorrelation, and values above 2 indicate negative autocorrelation.\n\n# Durbin-Watson test for autocorrelation in residuals\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\ndw_test &lt;- dwtest(simple_lm_result)\nprint(dw_test)\n\n\n    Durbin-Watson test\n\ndata:  simple_lm_result\nDW = 2.0848, p-value = 0.6637\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nDiagnostic Plot: Residual Plot\nA residual plot helps us check for the assumption of homoscedasticity, which assumes that the variance of the residuals is constant across all levels of the independent variable. We plot the residuals against the fitted values (predicted values) and look for patterns. If the plot shows a constant spread with no apparent pattern, it indicates that the model meets the assumption.\n\n# Create a residual plot\nplot(simple_lm_result$fitted.values, simple_lm_result$residuals,\n     main = \"Residual Plot\", xlab = \"Fitted Values\", ylab = \"Residuals\")\nabline(h = 0, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\nStatistical Test: Breusch-Pagan Test (for Homoscedasticity)\nThe Breusch-Pagan test is used to test for the assumption of homoscedasticity. The null hypothesis is that the variance of the residuals is constant (homoscedastic). A significant p-value suggests the presence of heteroscedasticity.\n\n# Breusch-Pagan test for homoscedasticity\nbp_test &lt;- bptest(simple_lm_result)\nprint(bp_test)\n\n\n    studentized Breusch-Pagan test\n\ndata:  simple_lm_result\nBP = 0.26677, df = 1, p-value = 0.6055\n\n\nRemember that diagnostic plots and statistical tests should be used as tools to assess the model’s validity and identify potential issues. It is crucial to interpret the results in conjunction with domain knowledge and the specific context of the analysis. Addressing any deviations from assumptions may involve data transformation or choosing a different model to improve the model’s performance and reliability.\n\n\nThis is still a work in progress…\n\nGeneralized linear model\nANOVA\nOne factor\nTwo factors\nANCOVA\nPCA\nDistance matrix and classification\nDAPC"
  },
  {
    "objectID": "projects/RMastery/index.html#graphical-visualisation",
    "href": "projects/RMastery/index.html#graphical-visualisation",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "Graphical visualisation",
    "text": "Graphical visualisation\nBase R\nggplot2"
  },
  {
    "objectID": "projects/RMastery/index.html#programming-in-r",
    "href": "projects/RMastery/index.html#programming-in-r",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "Programming in R",
    "text": "Programming in R\nConsequat elit laborum id eiusmod nisi ut minim officia. Tempor non consectetur veniam sit nisi nostrud Lorem minim quis amet anim duis. Sunt deserunt excepteur velit. Ipsum elit ullamco nostrud ad veniam et.\nDeserunt aute proident commodo nostrud. Esse veniam et duis ipsum adipisicing dolor incididunt. Labore deserunt anim commodo. Magna Lorem consectetur excepteur aliquip qui voluptate nulla velit Lorem Lorem minim nulla."
  },
  {
    "objectID": "projects/RMastery/index.html#reproducibility",
    "href": "projects/RMastery/index.html#reproducibility",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "Reproducibility",
    "text": "Reproducibility\n\n\nR version 4.3.2 (2023-10-31)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Sonoma 14.0\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Paris\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] lmtest_0.9-40   zoo_1.8-12      MASS_7.3-60     lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1  \n [7] dplyr_1.1.4     purrr_1.0.2     readr_2.1.4     tidyr_1.3.0     tibble_3.2.1    ggplot2_3.4.4  \n[13] tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.4      jsonlite_1.8.8    compiler_4.3.2    tidyselect_1.2.0  scales_1.3.0     \n [6] yaml_2.3.8        fastmap_1.1.1     lattice_0.21-9    R6_2.5.1          generics_0.1.3   \n[11] knitr_1.45        htmlwidgets_1.6.4 munsell_0.5.0     pillar_1.9.0      tzdb_0.4.0       \n[16] rlang_1.1.2       utf8_1.2.4        stringi_1.8.3     xfun_0.41         timechange_0.2.0 \n[21] cli_3.6.2         withr_2.5.2       magrittr_2.0.3    digest_0.6.33     grid_4.3.2       \n[26] hms_1.1.3         lifecycle_1.0.4   vctrs_0.6.5       evaluate_0.23     glue_1.6.2       \n[31] fansi_1.0.6       colorspace_2.1-0  rmarkdown_2.25    tools_4.3.2       pkgconfig_2.0.3  \n[36] htmltools_0.5.7"
  },
  {
    "objectID": "projects/RMastery/index.html#note",
    "href": "projects/RMastery/index.html#note",
    "title": "R Mastery: A Comprehensive Guide to Statistical Analysis, Data Visualization, and Programming Essentials",
    "section": "Note",
    "text": "Note\nThe writing process was conducted with the help of ChatGPT (OpenAI)."
  },
  {
    "objectID": "projects/Yersinia/index.html#sra-data-retrieval",
    "href": "projects/Yersinia/index.html#sra-data-retrieval",
    "title": "\nYersinia pestis dashboard",
    "section": "SRA Data Retrieval",
    "text": "SRA Data Retrieval\nIn this section, we’ll walk through the steps to retrieve sequence data from the Sequence Read Archive (SRA) using a Bash script (scr_SRA.sh). The script uses the SRA Toolkit to download and convert SRA files to FASTQ format.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_SRA.sh\n\nIn this script, change the job name, the email address and the two variables : Ref and Num_Accession.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_SRA\n#SBATCH --mem=99G\n#SBATCH --cpus-per-task=32\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\nRef=\"Spyrou_19\"\nNum_Accession=\"PRJEB29990\"\n\n# Load modules\nmodule load bioinfo/SRA-Toolkit/3.0.2\n\n# Create a new directory that will contain the sequences\nmkdir -p $Ref\ncd $Ref || exit # Checks if the directory already exists\n\n# Download SRA data\nprefetch $Num_Accession || { echo \"prefetch failed\"; exit 1; }\n\n# Bring .sra out of the intermediate files\nfind . -mindepth 2 -type f -exec mv -t . {} +\n\n# Convert SRA source files to fastQ sequence files\nfasterq-dump *.sra || { echo \"fasterq-dump failed\"; exit 1; }\n\n# Remove .sra files and empty sub-directories\nrm *.sra\nfind . -mindepth 1 -maxdepth 1 -type d -exec rm -r {} \\;\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_SRA.sh\nsbatch scr_SRA.sh\n\nYou need to separate single-end and paired-end files in different sub-directories. Use the following commands to do so.\n\ncd Spyrou_19\nmkdir ../Spyrou_19_paired\nmv *_1.fastq ../Spyrou_19_paired\nmv *_2.fastq ../Spyrou_19_paired\n\n# Rename the current directory Spyrou_19_single.\ncd ..\nmv Spyrou_19 Spyrou_19_single"
  },
  {
    "objectID": "projects/Yersinia/index.html#fastqc-analysis",
    "href": "projects/Yersinia/index.html#fastqc-analysis",
    "title": "\nYersinia pestis dashboard",
    "section": "FastQC Analysis",
    "text": "FastQC Analysis\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_fastqc.sh\n\nIn this script, change the job name, the email address and the directory.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_fastqc\n#SBATCH --mem=64G\n#SBATCH --cpus-per-task=80\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\n# Load module\nmodule load bioinfo/FastQC/0.12.1\nmodule load bioinfo/MultiQC/1.14\n\n# Change directory\ncd \"$(pwd)/Spyrou_19_paired\"\n\n# Launch analysis\nfor fichier in *.fastq\ndo\n    fastqc \"$fichier\"\ndone\n\nmultiqc . -o ..\n\nrm *fastqc.zip\nrm *fastqc.html\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_fastqc.sh\nsbatch scr_fastqc.sh\n\nOpen a new terminal window and execute the following command to retrieve the report from Genotoul.\n\nscp -r -p geranium@genobioinfo.toulouse.inrae.fr:/home/geranium/work/PJ_M2_ASO/Spyrou_19_paired/multiqc_report.html ."
  },
  {
    "objectID": "projects/Yersinia/index.html#seqtk-treatment",
    "href": "projects/Yersinia/index.html#seqtk-treatment",
    "title": "\nYersinia pestis dashboard",
    "section": "Seqtk treatment",
    "text": "Seqtk treatment\nTo effectively handle paired-end sequences fused into a singular sequence, it is imperative to employ the Seqtk to create new paired-end reads.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_seqtk.sh\n\nIn this script, change the job name, the email address and the two variables : Ref and sequence_length, you can find the sequence length in the MultiQC report.\n\n#!/bin/bash\n#SBATCH -J Bos_11_seqtk\n#SBATCH --mem=64G\n#SBATCH --cpus-per-task=80\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\n# Change this\nRef=\"Bos_11\"\nsequence_length=\"150\"\n\n# Move to the sequence directory\ncd \"$Ref\"\n\n# Trim length\ntrimlength=$((sequence_length / 2))\n\n# Create the folder for the output files\noutput_folder=\"${Ref}_seqtk\"\nmkdir -p \"$output_folder\"\n\n# Load the module\nmodule load bioinfo/Seqtk/1.3\n\n# Iterate through all FASTQ files in the directory\nfor file in *.fastq; do\n    # Extract the file name without the directory path\n    file_name=$(basename \"$file\")\n\n    # Remove the file name extension\n    base_name=$(echo \"$file_name\" | sed 's/\\.[^.]*$//')\n\n    # Apply seqtk and sed commands to each file\n    seqtk trimfq -b \"$trimlength\" \"$file\" | sed \"s/length=$sequence_length/length=$trimlength/g\" &gt; \"${output_folder}/${base_name}_trimmed_1.fastq\"\n    seqtk trimfq -e \"$trimlength\" \"$file\" | sed \"s/length=$sequence_length/length=$trimlength/g\" &gt; \"${output_folder}/${base_name}_trimmed_2.fastq\"\ndone\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_seqtk.sh\nsbatch scr_seqtk.sh"
  },
  {
    "objectID": "projects/Yersinia/index.html#paleomix-analysis",
    "href": "projects/Yersinia/index.html#paleomix-analysis",
    "title": "\nYersinia pestis dashboard",
    "section": "PALEOMIX Analysis",
    "text": "PALEOMIX Analysis\nReference Genome Preparation\nThis step is only necessary the first time you set up your analysis pipeline. It involves preparing the reference genome for subsequent analysis.\nNavigate to your project directory and execute the following commands.\n\n# Create a directory for Yersinia pestis reference files\nmkdir -p Ref_Ypestis\ncd Ref_Ypestis\n\n# Download the genomic sequence file from NCBI\nwget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/009/065/GCF_000009065.1_ASM906v1/GCF_000009065.1_ASM906v1_genomic.fna.gz\n\n# Unzip the downloaded file\ngunzip GCF_000009065.1_ASM906v1_genomic.fna.gz\n\n# Split the genomic sequence file into individual FASTA files\ncsplit -z -f GCF_000009065.1_ -b %02d.fna GCF_000009065.1_ASM906v1_genomic.fna \"/&gt;/\" '{*}'\n\n# Rename the split files for better identification\nmv GCF_000009065.1_00.fna GCF_000009065.1_chr.fasta\nmv GCF_000009065.1_01.fna GCF_000009065.1_pCD1.fasta\nmv GCF_000009065.1_02.fna GCF_000009065.1_pMT1.fasta\nmv GCF_000009065.1_03.fna GCF_000009065.1_pPCP1.fasta\n\n# Load required modules\nmodule load devel/Miniconda/Miniconda3\nmodule load bioinfo/PALEOMIX/1.3.8\n\n# Navigate to the home directory\ncd ~\n\n# Create a directory for JAR files and create a symbolic link to Picard JAR file\nmkdir -p ~/install/jar_root\nln -s /usr/local/bioinfo/src/Miniconda/Miniconda3/envs/paleomix-v1.3.7_env/share/picard-2.27.4-0/picard.jar ~/install/jar_root/\n\nPALEOMIX Analysis Script\nBefore generating the makefile for the analysis pipeline, it’s crucial to create a list of libraries. This step is essential for providing the necessary information about the libraries that will be processed in the subsequent analysis.\nPlease navigate to the directory where the sequences are located and execute the corresponding command.\nFor paired-end data, each library consists of two files: one for the forward reads (usually denoted as *_1.fastq) and another for the reverse reads (denoted as *_2.fastq). To facilitate the makefile creation, generate a list of libraries by specifying the corresponding file paths for each library.\n\nfor file in *_1.fastq; do\n    base=$(basename \"$file\" \"_1.fastq\")\n    echo \"    $base:\"\n    echo \"      Lane_$base: $(readlink -f ${base}_{Pair}.fastq)\"\ndone &gt; libraries.txt\n\nFor single-end data, each library consists of one file. To facilitate the makefile creation, generate a list of libraries by specifying the corresponding file paths for each library.\n\nfor file in *.fastq; do\n    base=$(basename \"$file\" \".fastq\")\n    echo \"    $base:\"\n    echo \"      Lane_$base: $(readlink -f ${base}.fastq)\"\ndone &gt; libraries.txt\n\nCopy the content of libraries.txt onto your computer and save it for the next step, ensuring to maintain proper indentation.\nNext, we generate the makefile, a configuration file for the PALEOMIX pipeline. The makefile contains instructions on how to process the data, including details about the reference genome, trimming parameters, and aligner settings. Generate the makefile within the directory where your earlier scripts are located.\n\n# Load modules\nmodule load devel/Miniconda/Miniconda3\nmodule load bioinfo/PALEOMIX/1.3.8\n\n# Retrieve the makefile template and modify it\npaleomix bam_pipeline makefile &gt; makefile.yaml\nnano makefile.yaml\n\nOnce you’ve generated the makefile.yaml, utilize the Nano text editor to access it for required modifications. Adjust the mapping quality (Phred) to 25 within the BWA options. Subsequently, insert the specified Prefixes (ensure to update the path accordingly) and the contents of libraries.txt in the designated section.\n\n# Settings for mappings performed using BWA\n    BWA:\n      # One of \"backtrack\", \"bwasw\", or \"mem\"; see the BWA documentation\n      # for a description of each algorithm (defaults to 'backtrack')\n      Algorithm: backtrack\n      # Filter aligned reads with a mapping quality (Phred) below this value\n      MinQuality: 25\n\n\n# Map of prefixes by name, each having a Path, which specifies the location of the\n# BWA/Bowtie2 index, and optional regions for which additional statistics are produced.\nPrefixes:\n  CO92_chromosome:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_chr.fasta\n  pCD1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pCD1.fasta\n  pMT1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pMT1.fasta\n  pPCP1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pPCP1.fasta\n\n\n# Mapping targets are specified using the following structure. Replace 'NAME_OF_TARGET'\n# with the desired prefix for filenames.\nSpyrou19_single:  # NAME_OF_TARGET\n  Spyrou19_single:  # NAME_OF_SAMPLE (same as NAME_OF_TARGET)\n  \n# Paste the content of libraries.txt here\n    ERR3457581:\n      Lane_ERR3457581: /work/user/geranium/PJ_M2_ASO/Spyrou_19_single/ERR3457581.fastq\n    ERR3457582:\n      Lane_ERR3457582: /work/user/geranium/PJ_M2_ASO/Spyrou_19_single/ERR3457582.fastq\n# ...\n\nPrior to proceeding to the subsequent stage, conduct a dry run with the following Bash command.\n\npaleomix bam_pipeline dryrun makefile.yaml\n\nPALEOMIX Execution Script\nIn this final step, we set up and execute the paleogenomic analysis script to process the prepared data using the configured analysis pipeline. The script, named scr_paleomix.sh, is responsible for defining job parameters and initiating the analysis on a high-performance computing (HPC) environment.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_paleomix.sh\n\nIn this script, change the job name, the email address, the output and the jar-root directories. Make sure that this script is in the same location as the makefile.yaml.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_paleomix\n#SBATCH --mem=99G\n#SBATCH --cpus-per-task=99\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\npaleomix bam_pipeline run --destination=Spyrou_19_paired_output --max-threads 99 --jre-option=\"-Xmx4g\" --jar-root=\"/home/geranium/install/jar_root/\" makefile.yaml\n\n\nchmod +x scr_paleomix.sh\nsbatch scr_paleomix.sh\n\nAs the final step, it is essential to review the summary file generated during the analysis. The summary file provides a comprehensive overview of key metrics, including read mapping statistics, coverage information, and potential post-mortem DNA damage assessments."
  },
  {
    "objectID": "projects/Yersinia/index.html#check-out-the-final-project",
    "href": "projects/Yersinia/index.html#check-out-the-final-project",
    "title": "Yersinia pestis dashboard",
    "section": "",
    "text": "To access the dashboard, simply click on this link.\nDetailed descriptions of the bioinformatic analyses can be found in the following tutorial."
  },
  {
    "objectID": "projects/Yersinia/index.html#yersinia-pestis-project-bioinformatics-tutorial",
    "href": "projects/Yersinia/index.html#yersinia-pestis-project-bioinformatics-tutorial",
    "title": "\nYersinia pestis dashboard",
    "section": "\nYersinia pestis project – Bioinformatics tutorial",
    "text": "Yersinia pestis project – Bioinformatics tutorial\nThis tutorial outlines the steps for SRA data retrieval, FastQC analysis, and PALEOMIX analysis, including reference genome preparation and execution scripts. Adjust the paths and parameters according to your specific setup.\nSRA Data Retrieval\nIn this section, we’ll walk through the steps to retrieve sequence data from the Sequence Read Archive (SRA) using a Bash script (scr_SRA.sh). The script uses the SRA Toolkit to download and convert SRA files to FASTQ format.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_SRA.sh\n\nIn this script, change the job name, the email address and the two variables : Ref and Num_Accession.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_SRA\n#SBATCH --mem=99G\n#SBATCH --cpus-per-task=32\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\nRef=\"Spyrou_19\"\nNum_Accession=\"PRJEB29990\"\n\n# Load modules\nmodule load bioinfo/SRA-Toolkit/3.0.2\n\n# Create a new directory that will contain the sequences\nmkdir -p $Ref\ncd $Ref || exit # Checks if the directory already exists\n\n# Download SRA data\nprefetch $Num_Accession || { echo \"prefetch failed\"; exit 1; }\n\n# Bring .sra out of the intermediate files\nfind . -mindepth 2 -type f -exec mv -t . {} +\n\n# Convert SRA source files to fastQ sequence files\nfasterq-dump *.sra || { echo \"fasterq-dump failed\"; exit 1; }\n\n# Remove .sra files and empty sub-directories\nrm *.sra\nfind . -mindepth 1 -maxdepth 1 -type d -exec rm -r {} \\;\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_SRA.sh\nsbatch scr_SRA.sh\n\nYou need to separate single-end and paired-end files in different sub-directories. Use the following commands to do so.\n\ncd Spyrou_19\nmkdir ../Spyrou_19_paired\nmv *_1.fastq ../Spyrou_19_paired\nmv *_2.fastq ../Spyrou_19_paired\n\n# Rename the current directory Spyrou_19_single.\ncd ..\nmv Spyrou_19 Spyrou_19_single\n\nFastQC Analysis\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_fastqc.sh\n\nIn this script, change the job name, the email address and the directory.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_fastqc\n#SBATCH --mem=64G\n#SBATCH --cpus-per-task=80\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\n# Load module\nmodule load bioinfo/FastQC/0.12.1\nmodule load bioinfo/MultiQC/1.14\n\n# Change directory\ncd \"$(pwd)/Spyrou_19_paired\"\n\n# Launch analysis\nfor fichier in *.fastq\ndo\n    fastqc \"$fichier\"\ndone\n\nmultiqc . -o ..\n\nrm *fastqc.zip\nrm *fastqc.html\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_fastqc.sh\nsbatch scr_fastqc.sh\n\nOpen a new terminal window and execute the following command to retrieve the report from Genotoul.\n\nscp -r -p geranium@genobioinfo.toulouse.inrae.fr:/home/geranium/work/PJ_M2_ASO/Spyrou_19_paired/multiqc_report.html .\n\nSeqtk treatment\nTo effectively handle paired-end sequences fused into a singular sequence, it is imperative to employ the Seqtk to create new paired-end reads.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_seqtk.sh\n\nIn this script, change the job name, the email address and the two variables : Ref and sequence_length, you can find the sequence length in the MultiQC report.\n\n#!/bin/bash\n#SBATCH -J Bos_11_seqtk\n#SBATCH --mem=64G\n#SBATCH --cpus-per-task=80\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\n# Change this\nRef=\"Bos_11\"\nsequence_length=\"150\"\n\n# Move to the sequence directory\ncd \"$Ref\"\n\n# Trim length\ntrimlength=$((sequence_length / 2))\n\n# Create the folder for the output files\noutput_folder=\"${Ref}_seqtk\"\nmkdir -p \"$output_folder\"\n\n# Load the module\nmodule load bioinfo/Seqtk/1.3\n\n# Iterate through all FASTQ files in the directory\nfor file in *.fastq; do\n    # Extract the file name without the directory path\n    file_name=$(basename \"$file\")\n\n    # Remove the file name extension\n    base_name=$(echo \"$file_name\" | sed 's/\\.[^.]*$//')\n\n    # Apply seqtk and sed commands to each file\n    seqtk trimfq -b \"$trimlength\" \"$file\" | sed \"s/length=$sequence_length/length=$trimlength/g\" &gt; \"${output_folder}/${base_name}_trimmed_1.fastq\"\n    seqtk trimfq -e \"$trimlength\" \"$file\" | sed \"s/length=$sequence_length/length=$trimlength/g\" &gt; \"${output_folder}/${base_name}_trimmed_2.fastq\"\ndone\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_seqtk.sh\nsbatch scr_seqtk.sh\n\nPALEOMIX Analysis\nReference Genome Preparation\nThis step is only necessary the first time you set up your analysis pipeline. It involves preparing the reference genome for subsequent analysis.\nNavigate to your project directory and execute the following commands.\n\n# Create a directory for Yersinia pestis reference files\nmkdir -p Ref_Ypestis\ncd Ref_Ypestis\n\n# Download the genomic sequence file from NCBI\nwget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/009/065/GCF_000009065.1_ASM906v1/GCF_000009065.1_ASM906v1_genomic.fna.gz\n\n# Unzip the downloaded file\ngunzip GCF_000009065.1_ASM906v1_genomic.fna.gz\n\n# Split the genomic sequence file into individual FASTA files\ncsplit -z -f GCF_000009065.1_ -b %02d.fna GCF_000009065.1_ASM906v1_genomic.fna \"/&gt;/\" '{*}'\n\n# Rename the split files for better identification\nmv GCF_000009065.1_00.fna GCF_000009065.1_chr.fasta\nmv GCF_000009065.1_01.fna GCF_000009065.1_pCD1.fasta\nmv GCF_000009065.1_02.fna GCF_000009065.1_pMT1.fasta\nmv GCF_000009065.1_03.fna GCF_000009065.1_pPCP1.fasta\n\n# Load required modules\nmodule load devel/Miniconda/Miniconda3\nmodule load bioinfo/PALEOMIX/1.3.8\n\n# Navigate to the home directory\ncd ~\n\n# Create a directory for JAR files and create a symbolic link to Picard JAR file\nmkdir -p ~/install/jar_root\nln -s /usr/local/bioinfo/src/Miniconda/Miniconda3/envs/paleomix-v1.3.7_env/share/picard-2.27.4-0/picard.jar ~/install/jar_root/\n\nPALEOMIX Analysis Script\nBefore generating the makefile for the analysis pipeline, it’s crucial to create a list of libraries. This step is essential for providing the necessary information about the libraries that will be processed in the subsequent analysis.\nPlease navigate to the directory where the sequences are located and execute the corresponding command.\nFor paired-end data, each library consists of two files: one for the forward reads (usually denoted as *_1.fastq) and another for the reverse reads (denoted as *_2.fastq). To facilitate the makefile creation, generate a list of libraries by specifying the corresponding file paths for each library.\n\nfor file in *_1.fastq; do\n    base=$(basename \"$file\" \"_1.fastq\")\n    echo \"    $base:\"\n    echo \"      Lane_$base: $(readlink -f ${base}_{Pair}.fastq)\"\ndone &gt; libraries.txt\n\nFor single-end data, each library consists of one file. To facilitate the makefile creation, generate a list of libraries by specifying the corresponding file paths for each library.\n\nfor file in *.fastq; do\n    base=$(basename \"$file\" \".fastq\")\n    echo \"    $base:\"\n    echo \"      Lane_$base: $(readlink -f ${base}.fastq)\"\ndone &gt; libraries.txt\n\nCopy the content of libraries.txt onto your computer and save it for the next step, ensuring to maintain proper indentation.\nNext, we generate the makefile, a configuration file for the PALEOMIX pipeline. The makefile contains instructions on how to process the data, including details about the reference genome, trimming parameters, and aligner settings. Generate the makefile within the directory where your earlier scripts are located.\n\n# Load modules\nmodule load devel/Miniconda/Miniconda3\nmodule load bioinfo/PALEOMIX/1.3.8\n\n# Retrieve the makefile template and modify it\npaleomix bam_pipeline makefile &gt; makefile.yaml\nnano makefile.yaml\n\nOnce you’ve generated the makefile.yaml, utilize the Nano text editor to access it for required modifications. Adjust the mapping quality (Phred) to 25 within the BWA options. Subsequently, insert the specified Prefixes (ensure to update the path accordingly) and the contents of libraries.txt in the designated section.\n\n# Settings for mappings performed using BWA\n    BWA:\n      # One of \"backtrack\", \"bwasw\", or \"mem\"; see the BWA documentation\n      # for a description of each algorithm (defaults to 'backtrack')\n      Algorithm: backtrack\n      # Filter aligned reads with a mapping quality (Phred) below this value\n      MinQuality: 25\n\n\n# Map of prefixes by name, each having a Path, which specifies the location of the\n# BWA/Bowtie2 index, and optional regions for which additional statistics are produced.\nPrefixes:\n  CO92_chromosome:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_chr.fasta\n  pCD1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pCD1.fasta\n  pMT1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pMT1.fasta\n  pPCP1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pPCP1.fasta\n\n\n# Mapping targets are specified using the following structure. Replace 'NAME_OF_TARGET'\n# with the desired prefix for filenames.\nSpyrou19_single:  # NAME_OF_TARGET\n  Spyrou19_single:  # NAME_OF_SAMPLE (same as NAME_OF_TARGET)\n  \n# Paste the content of libraries.txt here\n    ERR3457581:\n      Lane_ERR3457581: /work/user/geranium/PJ_M2_ASO/Spyrou_19_single/ERR3457581.fastq\n    ERR3457582:\n      Lane_ERR3457582: /work/user/geranium/PJ_M2_ASO/Spyrou_19_single/ERR3457582.fastq\n# ...\n\nPrior to proceeding to the subsequent stage, conduct a dry run with the following Bash command.\n\npaleomix bam_pipeline dryrun makefile.yaml\n\nPALEOMIX Execution Script\nIn this final step, we set up and execute the paleogenomic analysis script to process the prepared data using the configured analysis pipeline. The script, named scr_paleomix.sh, is responsible for defining job parameters and initiating the analysis on a high-performance computing (HPC) environment.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_paleomix.sh\n\nIn this script, change the job name, the email address, the output and the jar-root directories. Make sure that this script is in the same location as the makefile.yaml.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_paleomix\n#SBATCH --mem=99G\n#SBATCH --cpus-per-task=99\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\npaleomix bam_pipeline run --destination=Spyrou_19_paired_output --max-threads 99 --jre-option=\"-Xmx4g\" --jar-root=\"/home/geranium/install/jar_root/\" makefile.yaml\n\n\nchmod +x scr_paleomix.sh\nsbatch scr_paleomix.sh\n\nAs the final step, it is essential to review the summary file generated during the analysis. The summary file provides a comprehensive overview of key metrics, including read mapping statistics, coverage information, and potential post-mortem DNA damage assessments."
  },
  {
    "objectID": "projects/Yersinia/index.html#bioinformatics-tutorial",
    "href": "projects/Yersinia/index.html#bioinformatics-tutorial",
    "title": "Yersinia pestis dashboard",
    "section": "Bioinformatics tutorial",
    "text": "Bioinformatics tutorial\nThis tutorial outlines the steps for SRA data retrieval, FastQC analysis, and PALEOMIX analysis, including reference genome preparation and execution scripts. Adjust the paths and parameters according to your specific setup.\n\nSRA Data Retrieval\nIn this section, we’ll walk through the steps to retrieve sequence data from the Sequence Read Archive (SRA) using a Bash script (scr_SRA.sh). The script uses the SRA Toolkit to download and convert SRA files to FASTQ format.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_SRA.sh\n\nIn this script, change the job name, the email address and the two variables : Ref and Num_Accession.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_SRA\n#SBATCH --mem=99G\n#SBATCH --cpus-per-task=32\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\nRef=\"Spyrou_19\"\nNum_Accession=\"PRJEB29990\"\n\n# Load modules\nmodule load bioinfo/SRA-Toolkit/3.0.2\n\n# Create a new directory that will contain the sequences\nmkdir -p $Ref\ncd $Ref || exit # Checks if the directory already exists\n\n# Download SRA data\nprefetch $Num_Accession || { echo \"prefetch failed\"; exit 1; }\n\n# Bring .sra out of the intermediate files\nfind . -mindepth 2 -type f -exec mv -t . {} +\n\n# Convert SRA source files to fastQ sequence files\nfasterq-dump *.sra || { echo \"fasterq-dump failed\"; exit 1; }\n\n# Remove .sra files and empty sub-directories\nrm *.sra\nfind . -mindepth 1 -maxdepth 1 -type d -exec rm -r {} \\;\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_SRA.sh\nsbatch scr_SRA.sh\n\nYou need to separate single-end and paired-end files in different sub-directories. Use the following commands to do so.\n\ncd Spyrou_19\nmkdir ../Spyrou_19_paired\nmv *_1.fastq ../Spyrou_19_paired\nmv *_2.fastq ../Spyrou_19_paired\n\n# Rename the current directory Spyrou_19_single.\ncd ..\nmv Spyrou_19 Spyrou_19_single\n\n\n\nFastQC Analysis\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_fastqc.sh\n\nIn this script, change the job name, the email address and the directory.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_fastqc\n#SBATCH --mem=64G\n#SBATCH --cpus-per-task=80\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\n# Load module\nmodule load bioinfo/FastQC/0.12.1\nmodule load bioinfo/MultiQC/1.14\n\n# Change directory\ncd \"$(pwd)/Spyrou_19_paired\"\n\n# Launch analysis\nfor fichier in *.fastq\ndo\n    fastqc \"$fichier\"\ndone\n\nmultiqc . -o ..\n\nrm *fastqc.zip\nrm *fastqc.html\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_fastqc.sh\nsbatch scr_fastqc.sh\n\nOpen a new terminal window and execute the following command to retrieve the report from Genotoul.\n\nscp -r -p geranium@genobioinfo.toulouse.inrae.fr:/home/geranium/work/PJ_M2_ASO/Spyrou_19_paired/multiqc_report.html .\n\n\n\nSeqtk treatment\nTo effectively handle paired-end sequences fused into a singular sequence, it is imperative to employ the Seqtk to create new paired-end reads.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_seqtk.sh\n\nIn this script, change the job name, the email address and the two variables : Ref and sequence_length, you can find the sequence length in the MultiQC report.\n\n#!/bin/bash\n#SBATCH -J Bos_11_seqtk\n#SBATCH --mem=64G\n#SBATCH --cpus-per-task=80\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\n# Change this\nRef=\"Bos_11\"\nsequence_length=\"150\"\n\n# Move to the sequence directory\ncd \"$Ref\"\n\n# Trim length\ntrimlength=$((sequence_length / 2))\n\n# Create the folder for the output files\noutput_folder=\"${Ref}_seqtk\"\nmkdir -p \"$output_folder\"\n\n# Load the module\nmodule load bioinfo/Seqtk/1.3\n\n# Iterate through all FASTQ files in the directory\nfor file in *.fastq; do\n    # Extract the file name without the directory path\n    file_name=$(basename \"$file\")\n\n    # Remove the file name extension\n    base_name=$(echo \"$file_name\" | sed 's/\\.[^.]*$//')\n\n    # Apply seqtk and sed commands to each file\n    seqtk trimfq -b \"$trimlength\" \"$file\" | sed \"s/length=$sequence_length/length=$trimlength/g\" &gt; \"${output_folder}/${base_name}_trimmed_1.fastq\"\n    seqtk trimfq -e \"$trimlength\" \"$file\" | sed \"s/length=$sequence_length/length=$trimlength/g\" &gt; \"${output_folder}/${base_name}_trimmed_2.fastq\"\ndone\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_seqtk.sh\nsbatch scr_seqtk.sh\n\n\n\nPALEOMIX Analysis\n\nReference Genome Preparation\nThis step is only necessary the first time you set up your analysis pipeline. It involves preparing the reference genome for subsequent analysis.\nNavigate to your project directory and execute the following commands.\n\n# Create a directory for Yersinia pestis reference files\nmkdir -p Ref_Ypestis\ncd Ref_Ypestis\n\n# Download the genomic sequence file from NCBI\nwget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/009/065/GCF_000009065.1_ASM906v1/GCF_000009065.1_ASM906v1_genomic.fna.gz\n\n# Unzip the downloaded file\ngunzip GCF_000009065.1_ASM906v1_genomic.fna.gz\n\n# Split the genomic sequence file into individual FASTA files\ncsplit -z -f GCF_000009065.1_ -b %02d.fna GCF_000009065.1_ASM906v1_genomic.fna \"/&gt;/\" '{*}'\n\n# Rename the split files for better identification\nmv GCF_000009065.1_00.fna GCF_000009065.1_chr.fasta\nmv GCF_000009065.1_01.fna GCF_000009065.1_pCD1.fasta\nmv GCF_000009065.1_02.fna GCF_000009065.1_pMT1.fasta\nmv GCF_000009065.1_03.fna GCF_000009065.1_pPCP1.fasta\n\n# Load required modules\nmodule load devel/Miniconda/Miniconda3\nmodule load bioinfo/PALEOMIX/1.3.8\n\n# Navigate to the home directory\ncd ~\n\n# Create a directory for JAR files and create a symbolic link to Picard JAR file\nmkdir -p ~/install/jar_root\nln -s /usr/local/bioinfo/src/Miniconda/Miniconda3/envs/paleomix-v1.3.7_env/share/picard-2.27.4-0/picard.jar ~/install/jar_root/\n\n\n\nPALEOMIX Analysis Script\nBefore generating the makefile for the analysis pipeline, it’s crucial to create a list of libraries. This step is essential for providing the necessary information about the libraries that will be processed in the subsequent analysis.\nPlease navigate to the directory where the sequences are located and execute the corresponding command.\nFor paired-end data, each library consists of two files: one for the forward reads (usually denoted as *_1.fastq) and another for the reverse reads (denoted as *_2.fastq). To facilitate the makefile creation, generate a list of libraries by specifying the corresponding file paths for each library.\n\nfor file in *_1.fastq; do\n    base=$(basename \"$file\" \"_1.fastq\")\n    echo \"    $base:\"\n    echo \"      Lane_$base: $(readlink -f ${base}_{Pair}.fastq)\"\ndone &gt; libraries.txt\n\nFor single-end data, each library consists of one file. To facilitate the makefile creation, generate a list of libraries by specifying the corresponding file paths for each library.\n\nfor file in *.fastq; do\n    base=$(basename \"$file\" \".fastq\")\n    echo \"    $base:\"\n    echo \"      Lane_$base: $(readlink -f ${base}.fastq)\"\ndone &gt; libraries.txt\n\nCopy the content of libraries.txt onto your computer and save it for the next step, ensuring to maintain proper indentation.\nNext, we generate the makefile, a configuration file for the PALEOMIX pipeline. The makefile contains instructions on how to process the data, including details about the reference genome, trimming parameters, and aligner settings. Generate the makefile within the directory where your earlier scripts are located.\n\n# Load modules\nmodule load devel/Miniconda/Miniconda3\nmodule load bioinfo/PALEOMIX/1.3.8\n\n# Retrieve the makefile template and modify it\npaleomix bam_pipeline makefile &gt; makefile.yaml\nnano makefile.yaml\n\nOnce you’ve generated the makefile.yaml, utilize the Nano text editor to access it for required modifications. Adjust the mapping quality (Phred) to 25 within the BWA options. Subsequently, insert the specified Prefixes (ensure to update the path accordingly) and the contents of libraries.txt in the designated section.\n\n# Settings for mappings performed using BWA\n    BWA:\n      # One of \"backtrack\", \"bwasw\", or \"mem\"; see the BWA documentation\n      # for a description of each algorithm (defaults to 'backtrack')\n      Algorithm: backtrack\n      # Filter aligned reads with a mapping quality (Phred) below this value\n      MinQuality: 25\n\n\n# Map of prefixes by name, each having a Path, which specifies the location of the\n# BWA/Bowtie2 index, and optional regions for which additional statistics are produced.\nPrefixes:\n  CO92_chromosome:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_chr.fasta\n  pCD1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pCD1.fasta\n  pMT1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pMT1.fasta\n  pPCP1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pPCP1.fasta\n\n\n# Mapping targets are specified using the following structure. Replace 'NAME_OF_TARGET'\n# with the desired prefix for filenames.\nSpyrou19_single:  # NAME_OF_TARGET\n  Spyrou19_single:  # NAME_OF_SAMPLE (same as NAME_OF_TARGET)\n  \n# Paste the content of libraries.txt here\n    ERR3457581:\n      Lane_ERR3457581: /work/user/geranium/PJ_M2_ASO/Spyrou_19_single/ERR3457581.fastq\n    ERR3457582:\n      Lane_ERR3457582: /work/user/geranium/PJ_M2_ASO/Spyrou_19_single/ERR3457582.fastq\n# ...\n\nPrior to proceeding to the subsequent stage, conduct a dry run with the following Bash command.\n\npaleomix bam_pipeline dryrun makefile.yaml\n\n\n\nPALEOMIX Execution Script\nIn this final step, we set up and execute the paleogenomic analysis script to process the prepared data using the configured analysis pipeline. The script, named scr_paleomix.sh, is responsible for defining job parameters and initiating the analysis on a high-performance computing (HPC) environment.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_paleomix.sh\n\nIn this script, change the job name, the email address, the output and the jar-root directories. Make sure that this script is in the same location as the makefile.yaml.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_paleomix\n#SBATCH --mem=99G\n#SBATCH --cpus-per-task=99\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\npaleomix bam_pipeline run --destination=Spyrou_19_paired_output --max-threads 99 --jre-option=\"-Xmx4g\" --jar-root=\"/home/geranium/install/jar_root/\" makefile.yaml\n\n\nchmod +x scr_paleomix.sh\nsbatch scr_paleomix.sh\n\nAs the final step, it is essential to review the summary file generated during the analysis. The summary file provides a comprehensive overview of key metrics, including read mapping statistics, coverage information, and potential post-mortem DNA damage assessments."
  },
  {
    "objectID": "projects/Yersinia pestis/index.html",
    "href": "projects/Yersinia pestis/index.html",
    "title": "Yersinia pestis dashboard",
    "section": "",
    "text": "To access the dashboard, simply click on this link.\nDetailed descriptions of the bioinformatic analyses can be found in the following tutorial."
  },
  {
    "objectID": "projects/Yersinia pestis/index.html#check-out-the-final-project",
    "href": "projects/Yersinia pestis/index.html#check-out-the-final-project",
    "title": "Yersinia pestis dashboard",
    "section": "",
    "text": "To access the dashboard, simply click on this link.\nDetailed descriptions of the bioinformatic analyses can be found in the following tutorial."
  },
  {
    "objectID": "projects/Yersinia pestis/index.html#bioinformatics-tutorial",
    "href": "projects/Yersinia pestis/index.html#bioinformatics-tutorial",
    "title": "Yersinia pestis dashboard",
    "section": "Bioinformatics tutorial",
    "text": "Bioinformatics tutorial\nThis tutorial outlines the steps for SRA data retrieval, FastQC analysis, and PALEOMIX analysis, including reference genome preparation and execution scripts. Adjust the paths and parameters according to your specific setup.\n\nSRA Data Retrieval\nIn this section, we’ll walk through the steps to retrieve sequence data from the Sequence Read Archive (SRA) using a Bash script (scr_SRA.sh). The script uses the SRA Toolkit to download and convert SRA files to FASTQ format.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_SRA.sh\n\nIn this script, change the job name, the email address and the two variables : Ref and Num_Accession.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_SRA\n#SBATCH --mem=99G\n#SBATCH --cpus-per-task=32\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\nRef=\"Spyrou_19\"\nNum_Accession=\"PRJEB29990\"\n\n# Load modules\nmodule load bioinfo/SRA-Toolkit/3.0.2\n\n# Create a new directory that will contain the sequences\nmkdir -p $Ref\ncd $Ref || exit # Checks if the directory already exists\n\n# Download SRA data\nprefetch $Num_Accession || { echo \"prefetch failed\"; exit 1; }\n\n# Bring .sra out of the intermediate files\nfind . -mindepth 2 -type f -exec mv -t . {} +\n\n# Convert SRA source files to fastQ sequence files\nfasterq-dump *.sra || { echo \"fasterq-dump failed\"; exit 1; }\n\n# Remove .sra files and empty sub-directories\nrm *.sra\nfind . -mindepth 1 -maxdepth 1 -type d -exec rm -r {} \\;\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_SRA.sh\nsbatch scr_SRA.sh\n\nYou need to separate single-end and paired-end files in different sub-directories. Use the following commands to do so.\n\ncd Spyrou_19\nmkdir ../Spyrou_19_paired\nmv *_1.fastq ../Spyrou_19_paired\nmv *_2.fastq ../Spyrou_19_paired\n\n# Rename the current directory Spyrou_19_single.\ncd ..\nmv Spyrou_19 Spyrou_19_single\n\n\n\nFastQC Analysis\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_fastqc.sh\n\nIn this script, change the job name, the email address and the directory.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_fastqc\n#SBATCH --mem=64G\n#SBATCH --cpus-per-task=80\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\n# Load module\nmodule load bioinfo/FastQC/0.12.1\nmodule load bioinfo/MultiQC/1.14\n\n# Change directory\ncd \"$(pwd)/Spyrou_19_paired\"\n\n# Launch analysis\nfor fichier in *.fastq\ndo\n    fastqc \"$fichier\"\ndone\n\nmultiqc . -o ..\n\nrm *fastqc.zip\nrm *fastqc.html\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_fastqc.sh\nsbatch scr_fastqc.sh\n\nOpen a new terminal window and execute the following command to retrieve the report from Genotoul.\n\nscp -r -p geranium@genobioinfo.toulouse.inrae.fr:/home/geranium/work/PJ_M2_ASO/Spyrou_19_paired/multiqc_report.html .\n\n\n\nSeqtk treatment\nTo effectively handle paired-end sequences fused into a singular sequence, it is imperative to employ the Seqtk to create new paired-end reads.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_seqtk.sh\n\nIn this script, change the job name, the email address and the two variables : Ref and sequence_length, you can find the sequence length in the MultiQC report.\n\n#!/bin/bash\n#SBATCH -J Bos_11_seqtk\n#SBATCH --mem=64G\n#SBATCH --cpus-per-task=80\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\n# Change this\nRef=\"Bos_11\"\nsequence_length=\"150\"\n\n# Move to the sequence directory\ncd \"$Ref\"\n\n# Trim length\ntrimlength=$((sequence_length / 2))\n\n# Create the folder for the output files\noutput_folder=\"${Ref}_seqtk\"\nmkdir -p \"$output_folder\"\n\n# Load the module\nmodule load bioinfo/Seqtk/1.3\n\n# Iterate through all FASTQ files in the directory\nfor file in *.fastq; do\n    # Extract the file name without the directory path\n    file_name=$(basename \"$file\")\n\n    # Remove the file name extension\n    base_name=$(echo \"$file_name\" | sed 's/\\.[^.]*$//')\n\n    # Apply seqtk and sed commands to each file\n    seqtk trimfq -b \"$trimlength\" \"$file\" | sed \"s/length=$sequence_length/length=$trimlength/g\" &gt; \"${output_folder}/${base_name}_trimmed_1.fastq\"\n    seqtk trimfq -e \"$trimlength\" \"$file\" | sed \"s/length=$sequence_length/length=$trimlength/g\" &gt; \"${output_folder}/${base_name}_trimmed_2.fastq\"\ndone\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_seqtk.sh\nsbatch scr_seqtk.sh\n\n\n\nPALEOMIX Analysis\n\nReference Genome Preparation\nThis step is only necessary the first time you set up your analysis pipeline. It involves preparing the reference genome for subsequent analysis.\nNavigate to your project directory and execute the following commands.\n\n# Create a directory for Yersinia pestis reference files\nmkdir -p Ref_Ypestis\ncd Ref_Ypestis\n\n# Download the genomic sequence file from NCBI\nwget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/009/065/GCF_000009065.1_ASM906v1/GCF_000009065.1_ASM906v1_genomic.fna.gz\n\n# Unzip the downloaded file\ngunzip GCF_000009065.1_ASM906v1_genomic.fna.gz\n\n# Split the genomic sequence file into individual FASTA files\ncsplit -z -f GCF_000009065.1_ -b %02d.fna GCF_000009065.1_ASM906v1_genomic.fna \"/&gt;/\" '{*}'\n\n# Rename the split files for better identification\nmv GCF_000009065.1_00.fna GCF_000009065.1_chr.fasta\nmv GCF_000009065.1_01.fna GCF_000009065.1_pCD1.fasta\nmv GCF_000009065.1_02.fna GCF_000009065.1_pMT1.fasta\nmv GCF_000009065.1_03.fna GCF_000009065.1_pPCP1.fasta\n\n# Load required modules\nmodule load devel/Miniconda/Miniconda3\nmodule load bioinfo/PALEOMIX/1.3.8\n\n# Navigate to the home directory\ncd ~\n\n# Create a directory for JAR files and create a symbolic link to Picard JAR file\nmkdir -p ~/install/jar_root\nln -s /usr/local/bioinfo/src/Miniconda/Miniconda3/envs/paleomix-v1.3.7_env/share/picard-2.27.4-0/picard.jar ~/install/jar_root/\n\n\n\nPALEOMIX Analysis Script\nBefore generating the makefile for the analysis pipeline, it’s crucial to create a list of libraries. This step is essential for providing the necessary information about the libraries that will be processed in the subsequent analysis.\nPlease navigate to the directory where the sequences are located and execute the corresponding command.\nFor paired-end data, each library consists of two files: one for the forward reads (usually denoted as *_1.fastq) and another for the reverse reads (denoted as *_2.fastq). To facilitate the makefile creation, generate a list of libraries by specifying the corresponding file paths for each library.\n\nfor file in *_1.fastq; do\n    base=$(basename \"$file\" \"_1.fastq\")\n    echo \"    $base:\"\n    echo \"      Lane_$base: $(readlink -f ${base}_{Pair}.fastq)\"\ndone &gt; libraries.txt\n\nFor single-end data, each library consists of one file. To facilitate the makefile creation, generate a list of libraries by specifying the corresponding file paths for each library.\n\nfor file in *.fastq; do\n    base=$(basename \"$file\" \".fastq\")\n    echo \"    $base:\"\n    echo \"      Lane_$base: $(readlink -f ${base}.fastq)\"\ndone &gt; libraries.txt\n\nCopy the content of libraries.txt onto your computer and save it for the next step, ensuring to maintain proper indentation.\nNext, we generate the makefile, a configuration file for the PALEOMIX pipeline. The makefile contains instructions on how to process the data, including details about the reference genome, trimming parameters, and aligner settings. Generate the makefile within the directory where your earlier scripts are located.\n\n# Load modules\nmodule load devel/Miniconda/Miniconda3\nmodule load bioinfo/PALEOMIX/1.3.8\n\n# Retrieve the makefile template and modify it\npaleomix bam_pipeline makefile &gt; makefile.yaml\nnano makefile.yaml\n\nOnce you’ve generated the makefile.yaml, utilize the Nano text editor to access it for required modifications. Adjust the mapping quality (Phred) to 25 within the BWA options. Subsequently, insert the specified Prefixes (ensure to update the path accordingly) and the contents of libraries.txt in the designated section.\n\n# Settings for mappings performed using BWA\n    BWA:\n      # One of \"backtrack\", \"bwasw\", or \"mem\"; see the BWA documentation\n      # for a description of each algorithm (defaults to 'backtrack')\n      Algorithm: backtrack\n      # Filter aligned reads with a mapping quality (Phred) below this value\n      MinQuality: 25\n\n\n# Map of prefixes by name, each having a Path, which specifies the location of the\n# BWA/Bowtie2 index, and optional regions for which additional statistics are produced.\nPrefixes:\n  CO92_chromosome:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_chr.fasta\n  pCD1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pCD1.fasta\n  pMT1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pMT1.fasta\n  pPCP1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pPCP1.fasta\n\n\n# Mapping targets are specified using the following structure. Replace 'NAME_OF_TARGET'\n# with the desired prefix for filenames.\nSpyrou19_single:  # NAME_OF_TARGET\n  Spyrou19_single:  # NAME_OF_SAMPLE (same as NAME_OF_TARGET)\n  \n# Paste the content of libraries.txt here\n    ERR3457581:\n      Lane_ERR3457581: /work/user/geranium/PJ_M2_ASO/Spyrou_19_single/ERR3457581.fastq\n    ERR3457582:\n      Lane_ERR3457582: /work/user/geranium/PJ_M2_ASO/Spyrou_19_single/ERR3457582.fastq\n# ...\n\nPrior to proceeding to the subsequent stage, conduct a dry run with the following Bash command.\n\npaleomix bam_pipeline dryrun makefile.yaml\n\n\n\nPALEOMIX Execution Script\nIn this final step, we set up and execute the paleogenomic analysis script to process the prepared data using the configured analysis pipeline. The script, named scr_paleomix.sh, is responsible for defining job parameters and initiating the analysis on a high-performance computing (HPC) environment.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_paleomix.sh\n\nIn this script, change the job name, the email address, the output and the jar-root directories. Make sure that this script is in the same location as the makefile.yaml.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_paleomix\n#SBATCH --mem=99G\n#SBATCH --cpus-per-task=99\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\npaleomix bam_pipeline run --destination=Spyrou_19_paired_output --max-threads 99 --jre-option=\"-Xmx4g\" --jar-root=\"/home/geranium/install/jar_root/\" makefile.yaml\n\n\nchmod +x scr_paleomix.sh\nsbatch scr_paleomix.sh\n\nAs the final step, it is essential to review the summary file generated during the analysis. The summary file provides a comprehensive overview of key metrics, including read mapping statistics, coverage information, and potential post-mortem DNA damage assessments."
  },
  {
    "objectID": "projects/Yersinia_pestis/index.html",
    "href": "projects/Yersinia_pestis/index.html",
    "title": "Yersinia pestis dashboard",
    "section": "",
    "text": "To access the dashboard, simply click on this link.\nDetailed descriptions of the bioinformatic analyses can be found in the following tutorial."
  },
  {
    "objectID": "projects/Yersinia_pestis/index.html#check-out-the-final-project",
    "href": "projects/Yersinia_pestis/index.html#check-out-the-final-project",
    "title": "Yersinia pestis dashboard",
    "section": "",
    "text": "To access the dashboard, simply click on this link.\nDetailed descriptions of the bioinformatic analyses can be found in the following tutorial."
  },
  {
    "objectID": "projects/Yersinia_pestis/index.html#bioinformatics-tutorial",
    "href": "projects/Yersinia_pestis/index.html#bioinformatics-tutorial",
    "title": "Yersinia pestis dashboard",
    "section": "Bioinformatics tutorial",
    "text": "Bioinformatics tutorial\nThis tutorial outlines the steps for SRA data retrieval, FastQC analysis, and PALEOMIX analysis, including reference genome preparation and execution scripts. Adjust the paths and parameters according to your specific setup.\n\nSRA Data Retrieval\nIn this section, we’ll walk through the steps to retrieve sequence data from the Sequence Read Archive (SRA) using a Bash script (scr_SRA.sh). The script uses the SRA Toolkit to download and convert SRA files to FASTQ format.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_SRA.sh\n\nIn this script, change the job name, the email address and the two variables : Ref and Num_Accession.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_SRA\n#SBATCH --mem=99G\n#SBATCH --cpus-per-task=32\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\nRef=\"Spyrou_19\"\nNum_Accession=\"PRJEB29990\"\n\n# Load modules\nmodule load bioinfo/SRA-Toolkit/3.0.2\n\n# Create a new directory that will contain the sequences\nmkdir -p $Ref\ncd $Ref || exit # Checks if the directory already exists\n\n# Download SRA data\nprefetch $Num_Accession || { echo \"prefetch failed\"; exit 1; }\n\n# Bring .sra out of the intermediate files\nfind . -mindepth 2 -type f -exec mv -t . {} +\n\n# Convert SRA source files to fastQ sequence files\nfasterq-dump *.sra || { echo \"fasterq-dump failed\"; exit 1; }\n\n# Remove .sra files and empty sub-directories\nrm *.sra\nfind . -mindepth 1 -maxdepth 1 -type d -exec rm -r {} \\;\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_SRA.sh\nsbatch scr_SRA.sh\n\nYou need to separate single-end and paired-end files in different sub-directories. Use the following commands to do so.\n\ncd Spyrou_19\nmkdir ../Spyrou_19_paired\nmv *_1.fastq ../Spyrou_19_paired\nmv *_2.fastq ../Spyrou_19_paired\n\n# Rename the current directory Spyrou_19_single.\ncd ..\nmv Spyrou_19 Spyrou_19_single\n\n\n\nFastQC Analysis\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_fastqc.sh\n\nIn this script, change the job name, the email address and the directory.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_fastqc\n#SBATCH --mem=64G\n#SBATCH --cpus-per-task=80\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\n# Load module\nmodule load bioinfo/FastQC/0.12.1\nmodule load bioinfo/MultiQC/1.14\n\n# Change directory\ncd \"$(pwd)/Spyrou_19_paired\"\n\n# Launch analysis\nfor fichier in *.fastq\ndo\n    fastqc \"$fichier\"\ndone\n\nmultiqc . -o ..\n\nrm *fastqc.zip\nrm *fastqc.html\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_fastqc.sh\nsbatch scr_fastqc.sh\n\nOpen a new terminal window and execute the following command to retrieve the report from Genotoul.\n\nscp -r -p geranium@genobioinfo.toulouse.inrae.fr:/home/geranium/work/PJ_M2_ASO/Spyrou_19_paired/multiqc_report.html .\n\n\n\nSeqtk treatment\nTo effectively handle paired-end sequences fused into a singular sequence, it is imperative to employ the Seqtk to create new paired-end reads.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_seqtk.sh\n\nIn this script, change the job name, the email address and the two variables : Ref and sequence_length, you can find the sequence length in the MultiQC report.\n\n#!/bin/bash\n#SBATCH -J Bos_11_seqtk\n#SBATCH --mem=64G\n#SBATCH --cpus-per-task=80\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\n# Change this\nRef=\"Bos_11\"\nsequence_length=\"150\"\n\n# Move to the sequence directory\ncd \"$Ref\"\n\n# Trim length\ntrimlength=$((sequence_length / 2))\n\n# Create the folder for the output files\noutput_folder=\"${Ref}_seqtk\"\nmkdir -p \"$output_folder\"\n\n# Load the module\nmodule load bioinfo/Seqtk/1.3\n\n# Iterate through all FASTQ files in the directory\nfor file in *.fastq; do\n    # Extract the file name without the directory path\n    file_name=$(basename \"$file\")\n\n    # Remove the file name extension\n    base_name=$(echo \"$file_name\" | sed 's/\\.[^.]*$//')\n\n    # Apply seqtk and sed commands to each file\n    seqtk trimfq -b \"$trimlength\" \"$file\" | sed \"s/length=$sequence_length/length=$trimlength/g\" &gt; \"${output_folder}/${base_name}_trimmed_1.fastq\"\n    seqtk trimfq -e \"$trimlength\" \"$file\" | sed \"s/length=$sequence_length/length=$trimlength/g\" &gt; \"${output_folder}/${base_name}_trimmed_2.fastq\"\ndone\n\nOnce you’ve made the script executable using the chmod +x command, run the script.\n\nchmod +x scr_seqtk.sh\nsbatch scr_seqtk.sh\n\n\n\nPALEOMIX Analysis\n\nReference Genome Preparation\nThis step is only necessary the first time you set up your analysis pipeline. It involves preparing the reference genome for subsequent analysis.\nNavigate to your project directory and execute the following commands.\n\n# Create a directory for Yersinia pestis reference files\nmkdir -p Ref_Ypestis\ncd Ref_Ypestis\n\n# Download the genomic sequence file from NCBI\nwget ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/009/065/GCF_000009065.1_ASM906v1/GCF_000009065.1_ASM906v1_genomic.fna.gz\n\n# Unzip the downloaded file\ngunzip GCF_000009065.1_ASM906v1_genomic.fna.gz\n\n# Split the genomic sequence file into individual FASTA files\ncsplit -z -f GCF_000009065.1_ -b %02d.fna GCF_000009065.1_ASM906v1_genomic.fna \"/&gt;/\" '{*}'\n\n# Rename the split files for better identification\nmv GCF_000009065.1_00.fna GCF_000009065.1_chr.fasta\nmv GCF_000009065.1_01.fna GCF_000009065.1_pCD1.fasta\nmv GCF_000009065.1_02.fna GCF_000009065.1_pMT1.fasta\nmv GCF_000009065.1_03.fna GCF_000009065.1_pPCP1.fasta\n\n# Load required modules\nmodule load devel/Miniconda/Miniconda3\nmodule load bioinfo/PALEOMIX/1.3.8\n\n# Navigate to the home directory\ncd ~\n\n# Create a directory for JAR files and create a symbolic link to Picard JAR file\nmkdir -p ~/install/jar_root\nln -s /usr/local/bioinfo/src/Miniconda/Miniconda3/envs/paleomix-v1.3.7_env/share/picard-2.27.4-0/picard.jar ~/install/jar_root/\n\n\n\nPALEOMIX Analysis Script\nBefore generating the makefile for the analysis pipeline, it’s crucial to create a list of libraries. This step is essential for providing the necessary information about the libraries that will be processed in the subsequent analysis.\nPlease navigate to the directory where the sequences are located and execute the corresponding command.\nFor paired-end data, each library consists of two files: one for the forward reads (usually denoted as *_1.fastq) and another for the reverse reads (denoted as *_2.fastq). To facilitate the makefile creation, generate a list of libraries by specifying the corresponding file paths for each library.\n\nfor file in *_1.fastq; do\n    base=$(basename \"$file\" \"_1.fastq\")\n    echo \"    $base:\"\n    echo \"      Lane_$base: $(readlink -f ${base}_{Pair}.fastq)\"\ndone &gt; libraries.txt\n\nFor single-end data, each library consists of one file. To facilitate the makefile creation, generate a list of libraries by specifying the corresponding file paths for each library.\n\nfor file in *.fastq; do\n    base=$(basename \"$file\" \".fastq\")\n    echo \"    $base:\"\n    echo \"      Lane_$base: $(readlink -f ${base}.fastq)\"\ndone &gt; libraries.txt\n\nCopy the content of libraries.txt onto your computer and save it for the next step, ensuring to maintain proper indentation.\nNext, we generate the makefile, a configuration file for the PALEOMIX pipeline. The makefile contains instructions on how to process the data, including details about the reference genome, trimming parameters, and aligner settings. Generate the makefile within the directory where your earlier scripts are located.\n\n# Load modules\nmodule load devel/Miniconda/Miniconda3\nmodule load bioinfo/PALEOMIX/1.3.8\n\n# Retrieve the makefile template and modify it\npaleomix bam_pipeline makefile &gt; makefile.yaml\nnano makefile.yaml\n\nOnce you’ve generated the makefile.yaml, utilize the Nano text editor to access it for required modifications. Adjust the mapping quality (Phred) to 25 within the BWA options. Subsequently, insert the specified Prefixes (ensure to update the path accordingly) and the contents of libraries.txt in the designated section.\n\n# Settings for mappings performed using BWA\n    BWA:\n      # One of \"backtrack\", \"bwasw\", or \"mem\"; see the BWA documentation\n      # for a description of each algorithm (defaults to 'backtrack')\n      Algorithm: backtrack\n      # Filter aligned reads with a mapping quality (Phred) below this value\n      MinQuality: 25\n\n\n# Map of prefixes by name, each having a Path, which specifies the location of the\n# BWA/Bowtie2 index, and optional regions for which additional statistics are produced.\nPrefixes:\n  CO92_chromosome:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_chr.fasta\n  pCD1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pCD1.fasta\n  pMT1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pMT1.fasta\n  pPCP1:\n    Path: /home/geranium/work/PJ_M2_ASO/Ref_Ypestis/GCF_000009065.1_pPCP1.fasta\n\n\n# Mapping targets are specified using the following structure. Replace 'NAME_OF_TARGET'\n# with the desired prefix for filenames.\nSpyrou19_single:  # NAME_OF_TARGET\n  Spyrou19_single:  # NAME_OF_SAMPLE (same as NAME_OF_TARGET)\n  \n# Paste the content of libraries.txt here\n    ERR3457581:\n      Lane_ERR3457581: /work/user/geranium/PJ_M2_ASO/Spyrou_19_single/ERR3457581.fastq\n    ERR3457582:\n      Lane_ERR3457582: /work/user/geranium/PJ_M2_ASO/Spyrou_19_single/ERR3457582.fastq\n# ...\n\nPrior to proceeding to the subsequent stage, conduct a dry run with the following Bash command.\n\npaleomix bam_pipeline dryrun makefile.yaml\n\n\n\nPALEOMIX Execution Script\nIn this final step, we set up and execute the paleogenomic analysis script to process the prepared data using the configured analysis pipeline. The script, named scr_paleomix.sh, is responsible for defining job parameters and initiating the analysis on a high-performance computing (HPC) environment.\nNavigate to your project directory and create or open the script in the Nano text editor.\n\nnano scr_paleomix.sh\n\nIn this script, change the job name, the email address, the output and the jar-root directories. Make sure that this script is in the same location as the makefile.yaml.\n\n#!/bin/bash\n#SBATCH -J Spyrou_19_paleomix\n#SBATCH --mem=99G\n#SBATCH --cpus-per-task=99\n#SBATCH --mail-type=BEGIN,END,FAIL\n#SBATCH --mail-user=benjamin.tournan@univ-tlse3.fr\n\npaleomix bam_pipeline run --destination=Spyrou_19_paired_output --max-threads 99 --jre-option=\"-Xmx4g\" --jar-root=\"/home/geranium/install/jar_root/\" makefile.yaml\n\n\nchmod +x scr_paleomix.sh\nsbatch scr_paleomix.sh\n\nAs the final step, it is essential to review the summary file generated during the analysis. The summary file provides a comprehensive overview of key metrics, including read mapping statistics, coverage information, and potential post-mortem DNA damage assessments."
  }
]